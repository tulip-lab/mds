{
    "nbformat": 4, 
    "metadata": {
        "language_info": {
            "version": "3.5.2", 
            "nbconvert_exporter": "python", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "mimetype": "text/x-python", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "file_extension": ".py"
        }, 
        "kernelspec": {
            "language": "python", 
            "name": "python3", 
            "display_name": "Python 3.5 (Experimental) with Spark 1.6 (Unsupported)"
        }
    }, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 05: Deep Learning)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session D - Text Analytics (1) : Classification and Clustering\n\n**The purpose of this session is to introduce how to work with textual data which are tremendously produced everyday. In this practical session, we present the following topics:**\n\n1. Text pre-processing techniques, also called text normalization, which involves using a variety of techniques to convert raw text into well defined sequences of linguistic components that have standard structure and notation.\n\n2. Text classification or categorization which involves trying to organize text documents into various categories, based on inherent properties or attributes of each text document.\n\n3. Document clustering uses unsupervised ML algorithms to group the documents into various clusters.\n\n** References and additional reading and resources**\n- [Mining Twitter Data with Python: 7 parts](https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/)\n- [texttk -- Text Preprocessing in Python](https://github.com/fmpr/texttk)\n\n\n---\n\n\n\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## 1. Text Preprocessing ( or Normalizarion)", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "Machine learning (ML) algorithms usually work with input features that are numeric in nature. You need to clean, normalize, and pre-process the initial textual data. Textual data are usually in native raw format which is not well formatted and standardized. Text pre-processing involves using a variety of techniques to convert raw text into well-defined sequences of linguistic components that have standard structure and notation.\n\nIn this section, we introduce the most popular text pre-processing techniques used in text analytics:\n- Expanding contractions\n- Lemmatization\n- Removing special characters and symbols\n- Removing stopwords\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "We are starting with a small corpus including 3 documents:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "corpus=[\"The brown fox wasn't that quick and he couldn't win the race\", \n        \"Hey that's a great deal! I just bought  phones for 199\", \n        \"You'll learn a lot in the book. Python is an amazing language!\"]", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### 1.1 Expanding contractions", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "*Contractions* are shortened version of words or syllables, e.g., isn\u2019t, won\u2019t. We have created a vocabulary\nfor contractions and their corresponding expanded forms that you can access in the file **contractions.py** in a Python dictionary which is partly shown as \n<img src=\"https://github.com/tuliplab/mds/raw/master/Jupyter/image/contraction.PNG\", width=300>\n\nIn this snippet, we create a function called   ``expand_contractions`` which containes the function ``expanded_match`` to find each contraction that matches the ``regex`` pattern we create out of all the contractions in our ``contraction_mapping`` dictionary. On matching any contraction, we substitute it with its corresponding expanded version and retain the correct case of the word.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# this routine will expand the contration in texts using some pre-defined contractions and rules\n# see the list of English contractions here https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions. \n# You can add your own contraction to extend the list\n# The list of pre-defined contractions is stored in constractions.py file in CONTRACTION_MAP\nimport re # regular expression lib\n   \n# this function looks for each contraction and called above function\ndef expand_contractions(text, contraction_mapping):\n    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),flags=re.IGNORECASE|re.DOTALL)\n    \n    # this function returns each expanded contraction\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n\n        expanded_contraction = contraction_mapping.get(match)\\\n                                if contraction_mapping.get(match)\\\n                                else contraction_mapping.get(match.lower())  \n\n        expanded_contraction = first_char+expanded_contraction[1:]\n        return expanded_contraction\n    \n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text    ", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "For expanding contractions in the corpus, we only need to call the preceding function,``expand_contractions``, for each document in the corpus. You can see how each contraction has been correctly expanded in the output just like we expected it.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "from contractions import CONTRACTION_MAP\n\nn_corpus=[expand_contractions(doc,CONTRACTION_MAP) for doc in corpus]\nprint(n_corpus)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### 1.2 Lemmatization\nThe process of [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation) is to remove word affixes to get to a base form of the word. This base form is also known as the root word, also known as the lemma, which will always be present in the dictionary. The lemmatization process is usually based on the part-of-speech (POS) of each word. We use ``pos_tag`` package in Natural Language Toolkit (nltk) to indicate the POS for each word. The function ``pos_tag_text()`` is used to assign the POS for every token (word) in the given ``text``.  Note that in ``pos_tag_text()`` function, we use  ``tokenize_text90`` to separate the text into words.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from nltk import pos_tag\nfrom nltk.corpus import wordnet as wn\nimport nltk\n\n# Annotate text tokens with POS tags\ndef pos_tag_text(text):\n    \n    def to_wn_tags(pos_tag):\n        if pos_tag.startswith('J'):\n            return wn.ADJ\n        elif pos_tag.startswith('V'):\n            return wn.VERB\n        elif pos_tag.startswith('N'):\n            return wn.NOUN\n        elif pos_tag.startswith('R'):\n            return wn.ADV\n        else:\n            return None\n    \n    tagged_text = pos_tag(tokenize_text(text))\n    tagged_lower_text = [(word.lower(), to_wn_tags(pos_tag))\n                         for word, pos_tag in\n                         tagged_text]\n    return tagged_lower_text\n    \ndef tokenize_text(text):\n    tokens = nltk.word_tokenize(text) \n    tokens = [token.strip() for token in tokens]\n    return tokens", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "The main function is ``lemmatize_text()``, which takes in a body of text data and lemmatizes each word of the text based on its POS tag if it is present and then returns the lemmatized text back to the user.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# use the lemmatizer based on Wordnet dictionary\nfrom nltk.stem import WordNetLemmatizer\n\n# lemmatize text based on POS tags  \ndef lemmatize_text(text):\n    wnl = WordNetLemmatizer()\n    pos_tagged_text = pos_tag_text(text)\n    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag else word  for word, pos_tag in pos_tagged_text]\n    lemmatized_text = ' '.join(lemmatized_tokens)\n    return lemmatized_text\n", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "For lemmatizing  text in the corpus, we only need to call the ``lemmatize_text()`` function for each document in the corpus. You can see how each word is recoverd to its lemma.\n\n<img src=\"https://github.com/tuliplab/mds/raw/master/Jupyter/image/warning.png\" width=\"40\", align=\"left\"></img> When you run the code for the first time, you might get an error that ask for \n\n- ``Resource 'tokenizers/punkt/english.pickle' not found ``  OR\n- ``Resource 'taggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle' not found``\n\nYou can (remove comment and) run the following code to download the resources.\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# nltk.download('punkt')\n# nltk.download('averaged_perceptron_tagger')\n# nltk.download(\"wordnet\")", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "sn_corpus=[lemmatize_text((doc)) for doc in n_corpus]\nprint(sn_corpus)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### 1.3 Removing special characters and symbols", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "We remove special characters by tokenizing the text just so we can remove some of the tokens that are actually contractions, but we may have failed to remove them in our first step. We remove all special symbols defined in ``string.punctuation`` from our text using regular expression matches.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "import string\n\ndef remove_special_characters(text):\n    tokens = tokenize_text(text)\n    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n    filtered_text = ' '.join(filtered_tokens)\n    return filtered_text", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We now can apply this function to the text obtained in the previous step. Not that the symbol ``!`` has been removed.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "scrolled": true
            }, 
            "source": "rsn_corpus=[remove_special_characters(doc) for doc in n_corpus]\nprint(rsn_corpus)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### 1.4 Removing stopwords", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "[Stopwords](https://en.wikipedia.org/wiki/Stop_words) are words that have little or no significance. They are usually removed from text during processing so as to retain words having maximum significance and context. Stopwords are usually words that end up occurring the most if you aggregated any corpus of text based on singular tokens and checked their frequencies. In the following code, we use the (english) stopword list from NLTK library.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "stopword_list = nltk.corpus.stopwords.words('english')\ndef remove_stopwords(text):\n    tokens = tokenize_text(text)\n    filtered_tokens = [token for token in tokens if token not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We can filter stopwords out of our corpus.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "rrsn_corpus=[remove_stopwords(doc) for doc in rsn_corpus]\nprint(rrsn_corpus)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "###  1.5 Combining all steps to normalize corpus\nNow that we have all our functions defined, we can build our text normalization pipeline by chaining all these functions one after another. The following function implements this, where it takes in a corpus of text documents and normalizes them and\nreturns a normalized corpus of text documents. ", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# Combining all steps to normalize corpus\ndef normalize_corpus(corpus, tokenize=False):    \n    normalized_corpus = []    \n    for text in corpus: # we will process every document\n        text = expand_contractions(text, CONTRACTION_MAP)\n        text = lemmatize_text(text)\n        text = remove_special_characters(text)\n        text = remove_stopwords(text)\n        normalized_corpus.append(text)\n        if tokenize:\n            text = tokenize_text(text)\n            normalized_corpus.append(text)\n            \n    return normalized_corpus", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "scrolled": false
            }, 
            "source": "norm_corpus=normalize_corpus(corpus)\nprint(\"Before normalizing:\\n \", corpus)\nprint(\"\\nAfter normalizing:\\n \", norm_corpus)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "## 2. Feature Extraction for Text data", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "There are various feature-extraction techniques that can be applied on text data, but before we jump into then, let us consider what we mean by features. Why do we need them, and how they are useful? In a dataset, there are typically many data points. Usually the rows of the dataset and the columns are various features or properties of the dataset, with specific values for each row or observation. In ML terminology, features are unique, measurable attributes or properties for each observation or data point in a dataset. Features are usually numeric in nature and can be absolute numeric values or categorical features that can be encoded as binary features for each category in the list using a process called one-hot encoding. The process of extracting and selecting features is both art and science, and this process is called feature extraction or feature engineering.\n\nNow we will look at some feature-extraction concepts and techniques specially aligned towards text data. We will be talking about and implementing the following feature-extraction techniques:\n- Bag of Words model\n- TF-IDF model\n- Word vectorization models", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### 2.1 Bag of Words Model", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "The Bag of Words model is perhaps one of the simplest yet most powerful techniques to extract features from text documents. The essence of this model is to convert text documents into vectors such that each document is converted into a vector that represents the frequency of all the distinct words that are present in the document vector space for that specific document.\n\nThe following code snippet gives us a function that implements a Bag of Words\u2013based feature-extraction model:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from sklearn.feature_extraction.text import CountVectorizer\n\ndef bow_extractor(corpus, ngram_range=(1,1)):    \n    vectorizer = CountVectorizer(min_df=1, ngram_range=ngram_range)\n    features = vectorizer.fit_transform(corpus)\n    return vectorizer, features", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Before applyting BOW extractor, we need to normalize our corpus using the routine introduced in Secion 1.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "norm_corpus=normalize_corpus(corpus) # normalize corpus\n\nbow_vectorizer, bow_features = bow_extractor(norm_corpus) # call above routine to extract features\nfeatures = bow_features.todense() # feature matrix\nfeature_names = bow_vectorizer.get_feature_names() # feature names\nprint(feature_names)\nprint(features)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### 2.2 TF-IDF Model\nThe Bag of Words model is good, but the vectors are completely based on absolute\nfrequencies of word occurrences. This has some potential problems where words that\nmay tend to occur a lot across all documents in the corpus will have higher frequencies\nand will tend to overshadow other words that may not occur as frequently but may\nbe more interesting and effective as features to identify specific categories for the\ndocuments. This is where TF-IDF comes into the picture. TF-IDF stands for Term\nFrequency-Inverse Document Frequency, a combination of two metrics: term frequency\nand inverse document frequency. This technique was originally developed as a metric for\nranking functions for showing search engine results based on user queries and has come\nto be a part of information retrieval and text feature extraction now.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "The following code snippet shows an implementation of getting the tfidf-based feature vectors, considering we have our Bag of Words feature vectors we obtained in the previous section:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from sklearn.feature_extraction.text import TfidfTransformer\n   \ndef tfidf_transformer(bow_matrix):\n    transformer = TfidfTransformer(norm='l2',smooth_idf=True,use_idf=True)\n    tfidf_matrix = transformer.fit_transform(bow_matrix)\n    return transformer, tfidf_matrix", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Note that ``bow_features`` and ``feature_names`` are the BOW matrix and feature list created in the previous section. We also created a function called ``display_features`` to display the feature matrix.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "import numpy as np\nimport pandas as pd\ndef display_features(features, feature_names):\n    df = pd.DataFrame(data=features,columns=feature_names)\n    print(df)\n\ntfidf_trans, tdidf_features = tfidf_transformer(bow_features) # compute tf-idf for each word\nfeatures = np.round(tdidf_features.todense(), 2) # round value to 2 decimal places\ndisplay_features(features, feature_names) # using above function to disply feature matrix associated with feature names\n", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### 2.3 Word2vec models", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "We will be using the gensim library in our implementation, which is Python implementation for ``word2vec`` that provides several high-level interfaces for easily building these models. The basic idea is to provide a corpus of documents as input and get feature vectors for them as output. Internally, it constructs a vocabulary based on the input text documents and learns vector representations for words based on various techniques mentioned earlier, and once this is complete, it builds a model that can be used to extract word vectors for each word in a document. Using various techniques like average weighting or tfidf weighting, we can compute the averaged vector representation of a document using its word vectors.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "scrolled": false
            }, 
            "source": "import gensim\n\ntokenized_corpus = [nltk.word_tokenize(sentence) for sentence in corpus]\n# build the word2vec model on our training corpus\nmodel = gensim.models.Word2Vec(tokenized_corpus, size=10, window=10,min_count=2, sample=1e-3)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Once we build a model, we will define and implement two techniques of combining word vectors together in text documents based on certain weighing schemes. We will  implement two techniques mentioned as follows.\n- Averaged word vectors\n- TF-IDF weighted word vectors", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "#### Averaged Word Vectors", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "In this technique, we will use an average weighted word vectorization scheme, where for each text document we will extract all\nthe tokens of the text document, and for each token in the document we will capture the subsequent word vector if present in the vocabulary. We will sum up all the word vectors and divide the result by the total number of words matched in the vocabulary to get a final resulting averaged word vector representation for the text document.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# define function to average word vectors for a text document\ndef average_word_vectors(words, model, vocabulary, num_features):\n    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n    nwords = 0.\n    for word in words:\n        if word in vocabulary:\n            nwords = nwords + 1.\n            feature_vector = np.add(feature_vector, model[word])     \n    if nwords:\n        feature_vector = np.divide(feature_vector, nwords)\n    return feature_vector\n\n# generalize above function for a corpus of documents\ndef averaged_word_vectorizer(corpus, model, num_features):\n    vocabulary = set(model.wv.index2word)\n    features = [average_word_vectors(tokenized_sentence, model, vocabulary,num_features)\n                for tokenized_sentence in corpus]\n    return np.array(features)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "The following snippet shows our function in action on our sample corpora:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "avg_word_vec_features = averaged_word_vectorizer(corpus=tokenized_corpus,model=model,num_features=10)\nprint(np.round(avg_word_vec_features, 3))", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "#### TF-IDF Weighted Averaged Word Vectors", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "This section introduces a new and novel technique of weighing each matched word vector with the word TF-TDF score and summing up\nall the word vectors for a document and dividing it by the sum of all the TF-IDF weights of the matched words in the document. This would basically give us a TF-IDF weighted averaged word vector for each document.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# define function to compute tfidf weighted averaged word vector for a document\ndef tfidf_wtd_avg_word_vectors(words, tfidf_vector, tfidf_vocabulary, model, num_features):\n    word_tfidfs = [tfidf_vector[0, tfidf_vocabulary.get(word)]\n                   if tfidf_vocabulary.get(word)\n                   else 0 for word in words]\n    word_tfidf_map = {word:tfidf_val for word, tfidf_val in zip(words, word_tfidfs)}\n    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n    vocabulary = set(model.wv.index2word)\n    wts = 0.\n    for word in words:\n        if word in vocabulary:\n            word_vector = model[word]\n            weighted_word_vector = word_tfidf_map[word] * word_vector\n            wts = wts + word_tfidf_map[word]\n            feature_vector = np.add(feature_vector, weighted_word_vector)\n    if wts:\n        feature_vector = np.divide(feature_vector, wts)\n    return feature_vector\n\n#generalize above function for a corpus of documents\ndef tfidf_weighted_averaged_word_vectorizer(corpus, tfidf_vectors,tfidf_vocabulary, model, num_features):\n    docs_tfidfs = [(doc, doc_tfidf) for doc, doc_tfidf in zip(corpus, tfidf_vectors)]\n    features = [tfidf_wtd_avg_word_vectors(tokenized_sentence, tfidf, tfidf_vocabulary,model, num_features)\n                for tokenized_sentence, tfidf in docs_tfidfs]\n    return np.array(features)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from sklearn.feature_extraction.text import TfidfVectorizer\ndef tfidf_extractor(corpus, ngram_range=(1,1)):\n    vectorizer = TfidfVectorizer(min_df=1,norm='l2',smooth_idf=True,use_idf=True,ngram_range=ngram_range)\n    features = vectorizer.fit_transform(corpus)\n    return vectorizer, features", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We can see our implemented function in action on our sample corpora using the following snippet:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "tfidf_vectorizer, tdidf_features = tfidf_extractor(corpus)\n# get tfidf weights and vocabulary from earlier results and compute result\ncorpus_tfidf = tdidf_features\nvocab = tfidf_vectorizer.vocabulary_\nwt_tfidf_word_vec_features = tfidf_weighted_averaged_word_vectorizer(corpus=tokenized_corpus, tfidf_vectors=corpus_tfidf,\n                                                                     tfidf_vocabulary=vocab, model=model,num_features=10)\nprint(np.round(wt_tfidf_word_vec_features, 3))", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "In the subsequent sections, we will be putting everything together and applying it on some real-world data to build a multi-class text classification system. For this, we will be using the 20 newsgroups dataset available for download using scikit-learn. The 20 newsgroups dataset comprises around 18,000 newsgroups posts spread across 20 different categories or topics, thus\nmaking this a 20-class classification problem!", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## 3. Text Classification and Clustering with Real-world dataset\n### 3.1 Dataset: 20 Newsgroups\n\nThe 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. It was originally collected by Ken Lang, probably for his [Newsweeder: Learning to filter netnews](http://qwone.com/~jason/20Newsgroups/lang95.bib) paper, though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n\nThe data is organized into 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. *comp.sys.ibm.pc.hardware / comp.sys.mac.hardware*), while others are highly unrelated (e.g *misc.forsale / soc.religion.christian*). Here is a list of the 20 newsgroups, partitioned (more or less) according to subject matter:\n\n``comp.graphics\ncomp.os.ms-windows.misc\ncomp.sys.ibm.pc.hardware\ncomp.sys.mac.hardware\ncomp.windows.x\t\nrec.autos\nrec.motorcycles\nrec.sport.baseball\nrec.sport.hockey\t\nsci.crypt\nsci.electronics\nsci.med\nsci.space\nmisc.forsale\t\ntalk.politics.misc\ntalk.politics.guns\ntalk.politics.mideast\t\ntalk.religion.misc\nalt.atheism\nsoc.religion.christian``\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### Data preparation\nWe limit to use 5 classes of documents in this section to reduce processing time. Note that we will reuse the functions defined in Section 1 and 2. First, Let us start with loading the necessary dataset and defining functions for building the training and testing datasets:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "We define two functions: ``prepare_datasets()`` to split given documents into testing and training data; ``remove_empty_docs()`` to remove empty documents:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "def prepare_datasets(corpus, labels, test_data_proportion=0.3):\n    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels, test_size=0.33,random_state=42)\n    return train_X, test_X, train_Y, test_Y\n\ndef remove_empty_docs(corpus, labels):\n    filtered_corpus = []\n    filtered_labels = []\n    for doc, label in zip(corpus, labels):\n        if doc.strip():\n            filtered_corpus.append(doc)\n            filtered_labels.append(label)\n    return filtered_corpus, filtered_labels", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We can now get the data, see the total number of classes in our dataset, and split our data into training and test datasets using the following snippet (in case you do not have the data downloaded, feel free to connect to the Internet and take some time to download the complete corpus)", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "from sklearn.datasets import fetch_20newsgroups\n\ncategories = ['alt.atheism','rec.sport.baseball','talk.politics.mideast','comp.graphics', 'sci.space']\ndataset = fetch_20newsgroups(subset='all',categories=categories, remove=('headers', 'footers', 'quotes'))\n", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "# print all the classes\nprint(dataset.target_names)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# get corpus of documents and their corresponding labels\ncorpus, labels = dataset.data, dataset.target \ncorpus, labels = remove_empty_docs(corpus, labels)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "scrolled": true
            }, 
            "source": "# see sample document and its label index, name\nprint('Sample document:\\n', corpus[10])\nprint('Class label:\\n',labels[10])\nprint('Actual class label:\\n', dataset.target_names[labels[10]])\n", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "You can see from the preceding snippet how a sample document and label looks. Each document has its own class label, which is one of the 5 topics it is categorized into. The labels obtained are numbers, but we can easily map it back to the original category name if needed using the preceding snippet. We also split our data into train and test datasets, where the test dataset is 30 percent of the total data. We will build our model on the training data and test its performance on the test data.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from sklearn.model_selection import train_test_split\n# prepare train and test datasets\ntrain_corpus, test_corpus, train_labels, test_labels = prepare_datasets(corpus,labels, test_data_proportion=0.3)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "print(train_corpus[0])", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "print(test_corpus[0])", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Remember, a lot of normalization steps take place that we implemented earlier for each document in the corpora, so it may take some time to complete. Once we have normalized documents, we will use our feature extractor module built earlier to start extracting features from our documents. We will build models for Bag of Words, TF-IDF and compare their performances.\n\n<img src=\"https://github.com/tuliplab/mds/raw/master/Jupyter/image/warning.png\" width=\"40\", align=\"left\"></img> This step might take time, please be patient.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "norm_train_corpus = normalize_corpus(train_corpus)\nnorm_test_corpus = normalize_corpus(test_corpus)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from sklearn.feature_extraction.text import CountVectorizer\n\ndef bow_extractor(corpus):    \n    vectorizer = CountVectorizer()\n    features = vectorizer.fit_transform(corpus)\n    return vectorizer, features\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef tfidf_extractor(corpus, ngram_range=(1,1)):\n    vectorizer = TfidfVectorizer(min_df=1,norm='l2',smooth_idf=True,use_idf=True,ngram_range=ngram_range)\n    features = vectorizer.fit_transform(corpus)\n    return vectorizer, features", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# bag of words features\nbow_vectorizer, bow_train_features = bow_extractor(norm_train_corpus)\nbow_test_features = bow_vectorizer.transform(norm_test_corpus)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# tfidf features\ntfidf_vectorizer, tfidf_train_features = tfidf_extractor(norm_train_corpus)\ntfidf_test_features = tfidf_vectorizer.transform(norm_test_corpus)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### 3.2 Text Classification", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "Once we extract all the necessary features from our text documents using the preceding feature extractors, we define a function that will be useful for evaluation our classification models based on the four metrics discussed earlier, as shown in the following snippet", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from sklearn import metrics\nimport numpy as np\ndef get_metrics(true_labels, predicted_labels):\n    print('Accuracy:', np.round(metrics.accuracy_score(true_labels,predicted_labels),2))\n    print('Precision:', np.round(metrics.precision_score(true_labels,predicted_labels,average='weighted'),2))\n    print('Recall:', np.round(metrics.recall_score(true_labels,predicted_labels,average='weighted'),2))\n    print('F1 Score:', np.round(metrics.f1_score(true_labels,predicted_labels,average='weighted'),2))\n\ndef train_predict_evaluate_model(classifier, train_features, train_labels,test_features, test_labels):\n    # build model\n    classifier.fit(train_features, train_labels)\n    # predict using model\n    predictions = classifier.predict(test_features)\n    # evaluate model prediction performance\n    get_metrics(true_labels=test_labels,predicted_labels=predictions)\n    return predictions", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We now import two ML algorithms so that we can start building our models with them based on our extracted features.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "from sklearn.naive_bayes import MultinomialNB\n\nmnb = MultinomialNB()\nmnb.fit(bow_train_features, train_labels)\npredictions = mnb.predict(bow_test_features)\nprint(\"Multinomial Naive Bayes with bag of words features\")\nget_metrics(test_labels,predictions)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "from sklearn.naive_bayes import MultinomialNB\n\nmnb = MultinomialNB()\nmnb.fit(tfidf_train_features, train_labels)\npredictions = mnb.predict(tfidf_test_features)\nprint(\"Multinomial Naive Bayes with tfidf features\")\nget_metrics(test_labels,predictions)\n", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### 3.3 Text Clustering", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "Now we use all normalized documents to extract features:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "norm_corpus=normalize_corpus(corpus)\n\ntfidf_vectorizer, tfidf_features = tfidf_extractor(norm_corpus)\nbow_vectorizer, bow_features = bow_extractor(norm_corpus)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "from sklearn.cluster import KMeans\n\nnum_clusters = 5\nkm = KMeans(n_clusters=num_clusters)\nkm.fit(tfidf_features)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "print(\"Homogeneity (Purity): %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\nprint(\"Normalized mutual information: %0.3f\" % metrics.normalized_mutual_info_score(labels, km.labels_))\nprint(\"Adjusted Rand-Index: %0.3f\" % metrics.adjusted_rand_score(labels, km.labels_))\nprint(\"Silhouette Coefficient: %0.3f\" %metrics.silhouette_score(bow_features, km.labels_, sample_size=1000))", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We are now ready to analyze the cluster results of our k-means clustering. The following code snippet depicts the detailed analysis results for k-means clustering:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "vocab=bow_vectorizer.get_feature_names()\nprint(\"Top terms per cluster:\")\nprint()\n#sort cluster centers by proximity to centroid\norder_centroids = km.cluster_centers_.argsort()[:, ::-1] \n\nfor i in range(num_clusters):\n    print(\"Cluster %d words:\" % i, end=' ')\n    \n    for ind in order_centroids[i, :10]: #replace 6 with n words per cluster\n        print(' %s' % vocab[ind], end='')\n    print() #add whitespace\n    print() #add whitespace\n    \n", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "## 4. Exercises\n1. Use other 3 classifiers for text classification in Section 3.2 and report results.\n2. Use [AffinityPropagation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html) to cluster text documents in Section 3.3 and report results.\n3. Use Word2vec models to extract (2 kind of) features for classification and clustering in Section 3.2 and 3.3 and Exercises 1 and 2 and compare the results.\n4. Use 20newsgroups dataset in Section 3 with all categories and report the results.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }
    ], 
    "nbformat_minor": 2
}