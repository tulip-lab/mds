{
    "cells": [
        {
            "cell_type": "markdown", 
            "source": "# Modern Data Science \n**(Module 05: Deep Learning)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session A - Deep Learning with TensorFlow\n\n**The purpose of this session is to demonstrate how to work with an open source software library for developing deep neural networks apllications, called TensorFlow. In this practical session, we present the following topics:**\n\n1. What is TensorFlow\n2. How to install TensorFlow\n3. A quick tour of TensorFlow\n\n** References and additional reading and resources**\n- [Installing Tensorflow on Windows](https://www.tensorflow.org/install/install_windows)\n- [Tensorflow API documentations](https://www.tensorflow.org/versions/master/api_docs/python/)\n- [Examples with Tensorflow](https://www.tensorflow.org/versions/master/get_started/)\n---\n\n\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## <span style=\"color:#0b486b\">1. Getting started with TensorFlow</span>\n\nTensorFlow is a powerful open source software library for numerical computation, particularly well-suited for large-scale Machine Learning and highly-optimized for Deep Learning. Its basic principle is simple: you first define in Python a graph of computations to perform, and then TensorFlow takes that graph and runs it efficiently using optimized C++ code.\n\nTensorFlow has many advanced features. The **coolest** are:\n* ***Programmer friendly***: The front-end high-level Python API of TensorFlow offers much more flexibility (at the cost of higher complexity) to efficiently create all sorts of computations, including any neural network architecture you can come up with. In addition, there are many higher-level packages that serve as TensorFlow's wrappers to provide even simpler APIs such as TF.Learn ([http://tflearn.org/](http://tflearn.org/), compatible with Scikit-Learn), Keras ([https://keras.io/](https://keras.io/)), to name a few. You can use these APIs to train various types of neural networks in just a few lines of code.\n* ***Machine learner friendly***: TensorFlow automatically takes care of computing the gradients of the functions you define. This is called automatic differentiating (or `autodiff`).\n* ***TensorBoard***: TensorFlow also comes with a great visualization tool called *TensorBoard* that allows you to browse through the computation graph, view learning curves, and more. This feature is extremely useful for researchers.\n* ***Highly-optimized back-end***: TensorFlow includes highly efficient C++ implementations of many machine learning operations, particularly those needed to build neural networks. There is also a C++ API to define your own high-performance operations. * TensorFlow can train a network with millions of parameters on a training set composed of billions of instances with millions of features each.\n* ***Device switchable***: You can switch between computation on CPU and GPU with one line of code: `tf.device('cpu')` or `tf.device('gpu:x')`.\n* ***Parallelized and Distributed***: It is possible to break up the graph into several chunks and run them in parallel across multiple CPUs or GPUs. Moreover, TensorFlow also supports distributed computing, so you can train colossal neural networks on humongous training sets in a reasonable amount of time by splitting the computations across hundreds of servers.\n* ***Cross-platform***: TensorFlow can run not only on Windows, Linux, and macOS, but also on mobile devices, including both iOS and Android.\n* Last but not least, TensorFlow was developed by **Google Brain** team, and have been being long-term supported and maintained by **Google**. It powers many of Google\u2019s large-scale services, such as Google Cloud Speech, Google Photos, and Google Search.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## <span style=\"color:#0b486b\">2. Installation</span>\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 2.1 How to install TensorFlow", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "<span style=\"color:blue\">**Step 1.**</span> Install Anaconda (if you have Anaconda installed, you can skip this step)\n  \n<span style=\"color:blue\">**Step 2.**</span> Install TensorFlow on Windows with CPU:\n**`pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/windows/cpu/tensorflow-1.2.1-cp36-cp36m-win_amd64.whl`**\n\nIf you want to install Tensorflow on other OSs and/or with GPU, you can follow the instructions in [this guide](https://www.tensorflow.org/install/).", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### 2.2<span style=\"color:#0b486b\"> Testing the installation</span>", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "cell_type": "markdown", 
            "source": "<span style=\"color:blue\">**Step 1.**</span> Import TensorFlow and print out the version", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "import tensorflow as tf\nprint(tf.__version__)", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "<span style=\"color:blue\">**Step 2.**</span> Test TensorBoard", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "import tensorflow as tf\n\ng = tf.Graph()\n\nwith g.as_default():\n    a = tf.constant(5., name='a')\n    b = tf.constant(2., name='b')\n    c = tf.multiply(a, b, name='c')\n\nlog_dir = 'tf_logs/example00'\nwriter = tf.summary.FileWriter(log_dir, g)  # write the graph to a event file in folder log_dir\n\nwith tf.Session(graph=g) as sess:\n    print(sess.run(c))\n\nwriter.close()  # Close the writer when you're done with it", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "!ls tf_logs/example00", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "* Now open Anaconda prompt and change to the directory that contains your notebook (and the `tf_logs` folder) then type:  \n`tensorboard --logdir=tf_logs/example00`\n\n* The terminal will display: Starting Tensorboard b'47' at http://0.0.0.0:6006 or http://[computer_name]:6006\n* Open your browser and go to http://localhost:6006/. Click \"Graphs\" and you will see:  \n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example00/TensorBoard.jpg\", width=800>\n\n* If you want to open tensorboard at **another** port such as 9009, type the following in the terminal:  \n`tensorboard --logdir='tf_logs/example00' --port=9009`  \n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\" width=\"37\", align=\"left\"></img> *Some Windows users may have trouble with tensorboard, type the following in the terminal instead:*<br>\n`tensorboard --logdir=foo:tf_logs/example00`", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "## 2. A quick tour of TensorFlow", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "After Tensorflow is installed, we can import the TensorFlow as follows:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "import tensorflow as tf", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "Let's go with a very simple piece of code first!", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "x = tf.Variable(3, name=\"x\")\ny = tf.Variable(4, name=\"y\")\nf = x * x * y + y + 2", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "The code does not actually perform any computation yet. It just creates a computation graph. In fact, even the variables are not initialized yet.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### <span style=\"color:#0b486b\">2.1. TensorFlow session</span>\n\nTo evaluate this graph, you need to open a TensorFlow ***session*** and use it to initialize the variables and evaluate the function *`f`*. A TensorFlow session takes care of placing the operations onto devices such as CPUs and GPUs and running them, and it holds all the variable values.\n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\" width=\"20\", align=\"left\"></img> &nbsp; In distributed TensorFlow, variable values are stored on the servers instead of the session.\n\nThe following code creates a session, initializes the variables, evaluates function *`f`* then closes the session (which frees up resources):", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "sess = tf.Session()\nsess.run(x.initializer)\nsess.run(y.initializer)\nresult = sess.run(f)\nprint(result)\nsess.close()", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "* **`with`** block: If you don't want to repeat *`sess.run()`* all the time, you can use *`with`* statement as below:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "with tf.Session() as sess:\n    x.initializer.run()\n    y.initializer.run()\n    result = f.eval()\n    print(result)", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "Inside the *`with`* block, the session is set as the default session. Calling *`x.initializer.run()`* is equivalent to calling *`tf.get_default_session().run(x.initializer)`*, and similarly *`f.eval()`* is equivalent to calling *`tf.get_default_session().run(f)`*. This makes the code easier to read.<br> \n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\" width=\"20\", align=\"left\"></img> &nbsp; We don't need to call *`sess.close()`* here because the *`with`* context manager does it automatically.\n\n* **Interactive session**: Alternatively, you can even get rid of *`with`* block by creating an interactive session (*`InteractiveSession`*) when you are working with Jupyter notebook or a Python shell. The only difference from a regular Session is that when an *`InteractiveSession`* is created, it automatically sets itself as the default session. However, you do need to close the session manually when you are done.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "sess = tf.InteractiveSession()\nx.initializer.run()\ny.initializer.run()\nprint(f.eval())\nsess.close()", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "* **Global variables initialization**: Instead of manually running the initializer for every single variable, you can use the *`global_variables_initializer()`* function, and run it.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "init = tf.global_variables_initializer()  # prepare an init node\nwith tf.Session() as sess:\n    init.run()  # actually initialize all the variables\n    result = f.eval()\n    print(result)", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "### <span style=\"color:#0b486b\">2.2. Managing TensorFlow graphs</span>\n\nAny node you create is automatically added to the default graph. ", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "x1 = tf.Variable(1)\nx1.graph is tf.get_default_graph()", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "In most cases this is fine, but sometimes you may want to manage multiple independent graphs. You can do this by creating a new *`Graph`* and temporarily making it the default graph inside a *`with`* block, like so:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "graph = tf.Graph()\nwith graph.as_default():\n    x2 = tf.Variable(2)\nx2.graph is graph", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "x2.graph is tf.get_default_graph()", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "Sometime the default graph could have duplicated nodes (variables). We can reset to clean the graph as follows:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "tf.reset_default_graph()", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "### <span style=\"color:#0b486b\">2.3. Lifecycle of a node value</span>\n\nWhen you evaluate a node, TensorFlow automatically determines the set of nodes that it depends on and it evaluates these nodes first. Let's look at the following code:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "w = tf.constant(3)\nx = w + 2\ny = x + 5\nz = x * 3\nwith tf.Session() as sess:\n    print(y.eval())  # 10\n    print(z.eval())  # 15", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "The code evaluates *`y`* and *`z`*. For *`y`*, TensorFlow automatically detects that *`y`* depends on *`x`*, which depends on *`w`*, so it first evaluates *`w`*, then *`x`*, and then *`y`*. The schedule is: *`w -> x -> y`*. Likewise, the order for evaluating *`z`* is: *`w -> x -> z`*. It is important to note that it will ***not reuse*** the result of the previous evaluation of *`w`* and *`x`*. In short, the preceding code evaluates *`w`* and *`x`* ***twice***.<br>\n\nAll node values are dropped between graph runs, except variable values, which are maintained by the session across graph runs. A variable starts its life when its initializer is run, and it ends when the session is closed.\n\nIf you want to evaluate *`y`* and *`z`* more efficiently, i.e., without evaluating *`w`* and *`x`* twice as in the previous code, you must ask TensorFlow to simultaneously evaluate both *`y`* and *`z`* in just one graph run as follows:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "with tf.Session() as sess:\n    y_val, z_val = sess.run([y, z])\n    print(y_val)  # 10\n    print(z_val)  # 15", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "Sometimes, we will have two **independent** operations (ops) but you\u2019d like to specify which operation (op) should be run **first**. You can create context manager that specifies control dependencies for all operations constructed within the context. For example, the following codes call for an increment of *`global_step`* each time we compute *`learning_rate`*:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "tf.reset_default_graph()\n\nstarter_lr = 1.\ndecay_rate = 0.9\nglobal_step = tf.Variable(0., trainable=0)\nincr = tf.assign(global_step, global_step + 1)\n\nwith tf.control_dependencies([incr]):\n    learning_rate = starter_lr * tf.pow(decay_rate, global_step)\n    \nwith tf.Session() as sess:\n    global_step.initializer.run()\n    for i in range(5):\n        print('Global Step %d: Learning rate = %f' % (global_step.eval(), learning_rate.eval()))", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "**Without** controlling dependences: *`global_step`* will stay at `0.0`, and *`learning_rate`* will be `1.0`:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "tf.reset_default_graph()\n\nstarter_lr = 1.\ndecay_rate = 0.9\nglobal_step = tf.Variable(0., trainable=0)\nincr = tf.assign(global_step, global_step + 1)\n\nlearning_rate = starter_lr * tf.pow(decay_rate, global_step)\n    \nwith tf.Session() as sess:\n    global_step.initializer.run()\n    for i in range(5):\n        print('Global step %d: Learning rate = %f' % (global_step.eval(), learning_rate.eval()))\n        ", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "### <span style=\"color:#0b486b\">2.4. Placehoder</span>\n\nSuppose that you are evaluating a function *`y`* that takes an input *`x`*. You want to do this many times and change the input *`x`* at each time (as in an iterative algorithm when we want to replace the input data at every iteration). The simplest way to do this is to use *`placeholder`* nodes. These nodes are special because they don\u2019t actually perform any computation, they\njust output the data you tell them to output at runtime. They are typically used to pass the training data to TensorFlow during training. Let's look at a simple example:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "import numpy as np\nimport tensorflow as tf\ntf.reset_default_graph()\n\nx = tf.placeholder(tf.float32, shape=[None, 3])\ny = x + 2\nwith tf.Session() as sess:\n    print(y.eval(feed_dict={x: np.ones([1, 3])}))  #  feed 1x3 array \n    print(y.eval(feed_dict={x: np.zeros([2, 3])})) #  feed 2x3 array ", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "If you don\u2019t specify a value at runtime for a placeholder, you get an exception. Try this:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "with tf.Session() as sess:\n    print(y.eval())", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "### <span style=\"color:#0b486b\">2.5. Save and restore models</span>", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "Once you have trained your model, you should save its parameters to disk so that you can come back to it whenever you want, use it in another program, compare it to other models, and so on. Moreover, you probably want to save checkpoints at regular intervals during training so that if your computer crashes or encounters power-outage during training you can continue from the last checkpoint rather than start over from scratch.\n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\" width=\"20\", align=\"left\"></img> You might get into error if the folder path `models/example01` does not exist. You can create this folder in the directory containing this notebook.\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "tf.reset_default_graph()\n\ntheta = tf.Variable(tf.zeros([5]), name='theta')\ntrain_op = tf.assign(theta, theta + 1.)\n\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()\n\nn_epochs = 200\nwith tf.Session() as sess:\n    sess.run(init)\n    for epoch in range(n_epochs):\n        sess.run(train_op)\n        if (epoch + 1) % 100 == 0:  # checkpoint every 100 epochs\n            saver.save(sess, \"models/example01/save_and_restore.ckpt\")\n        \n    best_theta = theta.eval()\n    print(best_theta)", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "Restoring a model is just as easy: you create a `Saver` at the end of the construction phase just like before, but then at the beginning of the execution phase, instead of initializing the variables using the `init` node, you call the `restore()` method of the `Saver` object:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "tf.reset_default_graph()\n\ntheta = tf.Variable(tf.zeros([5]), name='theta')\ntrain_op = tf.assign(theta, theta + 1.)\n\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()\n\nn_epochs = 200\nwith tf.Session() as sess:\n    saver.restore(sess, \"models/example01/save_and_restore.ckpt\")\n    for epoch in range(n_epochs):\n        sess.run(train_op)\n        if (epoch + 1) % 100 == 0:  # checkpoint every 100 epochs\n            saver.save(sess, \"models/example01/save_and_restore_cont.ckpt\")\n        \n    best_theta = theta.eval()\n    print(best_theta)", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "After restoring and training for another 200 steps, best_theta is now [400.  400.  400.  400.  400.]", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### <span style=\"color:#0b486b\">2.6. Visualize computational graph and learning curves in TensorBoard</span>", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "Normally, we rely on the `print()` function and `matplotlib` to visualize progress during training. There is a better way, i.e., using TensorBoard. If you feed it some training stats, it will display nice interactive visualizations of\nthese stats in your web browser (e.g., learning curves). You can also provide it the graph\u2019s definition and\nit will give you a great interface to browse through it. This is very useful to identify errors in the graph, to\nfind bottlenecks, and so on. Let's visualize learning rate and global step in the example above.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "# construction\ntf.reset_default_graph()\n\nstarter_lr = 1.\ndecay_rate = 0.9\nglobal_step = tf.Variable(0., trainable=0)\nincr = tf.assign(global_step, global_step + 1)\n\nwith tf.control_dependencies([incr]):\n    learning_rate = starter_lr * tf.pow(decay_rate, global_step)", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "Here, we construct the graph as normal:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "tf.summary.scalar('learning_rate', learning_rate)\ntf.summary.scalar('global_step', global_step)\nmerged = tf.summary.merge_all() # Merges all summaries collected in the default graph", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "The first two lines create two summary *`ops`* in the graph that will evaluate the *`learning_rate`* and *`global_step`* value and write them to a TensorBoard compatible binary log string called a summary. The third line creates a node that merges all summaries collected in the default graph. In the execution phase, you'll need to evaluate the merged node regularly during training (e.g., every 10 mini-batches). This will output a summary that you can then write to the events file using the *`file_writer`*.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "import time\nlogdir = \"tf_logs/example01/model-at-{}\".format(time.strftime('%Y-%m-%d_%H.%M.%S'))\nfile_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "You need to use a different log directory every time you run your program, or else TensorBoard will merge stats from different runs, which will mess up the visualizations. The simplest solution for this is to include a timestamp in the log directory name. Now's the execution phase:", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "The first line creates a node in the graph that will evaluate the *MSE* value and write it to a TensorBoard compatible binary log string called a summary. Then you need to update the execution phase to evaluate the *`mse_summary`* node regularly during training\n(e.g., every 10 mini-batches). This will output a summary that you can then write to the events file using\nthe *`file_writer`*. Here is the updated code:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "with tf.Session() as sess:\n    global_step.initializer.run()\n    for i in range(50):\n        merged_ = merged.eval()\n        file_writer.add_summary(merged_, i + 1)", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/warning.png\" width=\"40\", align=\"left\"></img> In actual Deep Learning implementation logging training stats at every single training step, as this would significantly slow down training. Instead, you should log 200 iterations for example.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "Finally, you want to close the FileWriter at the end of the program:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "file_writer.close()", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "Great! Now it\u2019s time to fire up the TensorBoard server. You need to activate your virtual environment\nif you created one, then start the server by running the *`tensorboard`* command, pointing it to the root log\ndirectory. This starts the TensorBoard web server, listening on port 6006 (which is \u201cgoog\u201d written upside\ndown :) )\nNext open a browser and go to http://0.0.0.0:6006/ (or http://localhost:6006/). Welcome to\nTensorBoard! In the Scalars tab, you'll see *`global_step`* and *`learning_rate`*:\n\n<img src='https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example01/learning_rate.png' width=300>", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### <span style=\"color:#0b486b\">II.7. Name Scopes</span>\n\nWhen dealing with more complex models such as neural networks, the graph can easily become cluttered with thousands of nodes. To avoid this, you can create name scopes to group related nodes. The following code defines two operations sum and product under two scopes:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "tf.reset_default_graph()\n\na = tf.placeholder(tf.float32, shape=(), name='a')\nb = tf.placeholder(tf.float32, shape=(), name='b')\n\nwith tf.name_scope('calc'):\n    c = a + b\n    d = a + b\n    e = a * b\n    f = tf.add(a, b, name='sum')", 
            "outputs": []
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "print(c.op.name)\nprint(d.op.name)\nprint(e.op.name)\nprint(f.op.name)", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "The name of each op defined within the scope is now prefixed with \"calc/\". `c`, `d`, and `e` get generic names while `d` gets the name 'sum' that we passed into the operation. Note that `d` and `c` has the same generic name *`add`* but `d` is defined created later so it get the name *`calc/add_1`*.\n\nFor more detail about *`tf.name_scope`*, read: https://www.tensorflow.org/api_docs/python/tf/Graph#name_scope.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### <span style=\"color:#0b486b\">II.8. Modularity</span>\n\nSuppose you want to create a graph that adds the output of two [rectified linear units (ReLU)](https://en.wikipedia.org/wiki/Rectifier_(neural_networks). A ReLU computes a linear function of the inputs, and outputs the result if it is positive, and 0 otherwise, as shown in the following equation:\n\n$$h_{\\mathbf{W}, \\mathbf{b}}(\\mathbf{x})=\\max(\\mathbf{W}^T\\mathbf{x} + \\mathbf{b}, 0)$$\n\nThe following does the job but quit repetitive:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "tf.reset_default_graph()\n\nn_features = 3\nX = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n\nw1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\nw2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n\nb1 = tf.Variable(0.0, name=\"bias1\")\nb2 = tf.Variable(0.0, name=\"bias2\")\n\nz1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\nz2 = tf.add(tf.matmul(X, w2), b2, name=\"z2\")\n\nrelu1 = tf.maximum(z1, 0., name=\"relu1\")\nrelu2 = tf.maximum(z1, 0., name=\"relu2\")\n\noutput = tf.add(relu1, relu2, name=\"output\")\n\nlogdir = 'tf_logs/example01/modularity'\nfile_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\nfile_writer.close()", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "<img src='images/example01/graph01.PNG' width=500>\n\nThe graph looks unorganized and hard to follow! Suppose we want to creates many ReLUs and outputs their sum, the graph is very messy and is hard to follow.\n\nThe following code creates five ReLUs and outputs their sum (note that *`add_n()`* creates an operation that will compute the sum of a list of tensors):", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "tf.reset_default_graph()\n\ndef relu(X):\n    w_shape = (int(X.get_shape()[1]), 1)\n    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n    b = tf.Variable(0.0, name=\"bias\")\n    z = tf.add(tf.matmul(X, w), b, name=\"linear\")\n    return tf.maximum(z, 0., name=\"relu\")\n\nn_features = 3\nX = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\nrelus = [relu(X) for i in range(5)]\noutput = tf.add_n(relus, name=\"output\")\n\nlogdir = 'tf_logs/example01/modularity_clean'\nfile_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\nfile_writer.close()", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "Note that when you create a node, TensorFlow checks whether its name already exists, and if it does, TensorFlow appends an underscore followed by an index to make the name unique. So the first ReLU contains nodes named \"weights\", \"bias\", \"z\", and \"relu\" (plus many more nodes with their default name, such as \"MatMul\"); the second ReLU contains nodes named \"weights_1\", \"bias_1\", and so on; the third ReLU contains nodes named \"weights_2\", \"bias_2\", and so on. TensorBoard identifies such series and collapses them together to reduce clutter.\n\n<img src='https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example01/graph02.PNG' width=500>", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "Let's try using name scopes to see if we can make the graph clearer!", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "tf.reset_default_graph()\n\ndef relu(X):\n    with tf.name_scope('relu'):\n        w_shape = (int(X.get_shape()[1]), 1)\n        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n        b = tf.Variable(0.0, name=\"bias\")\n        z = tf.add(tf.matmul(X, w), b, name=\"linear\")\n        return tf.maximum(z, 0., name=\"relu\")\n\nn_features = 3\nX = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\nrelus = [relu(X) for i in range(5)]\noutput = tf.add_n(relus, name=\"output\")\n\nlogdir = 'tf_logs/example01/modularity_clearer'\nfile_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\nfile_writer.close()", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "<img src='https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example01/graph03.PNG' width=600>\n\nThe graph now looks much clearer. Notice that TensorFlow also gives the name scopes unique names by appending _1, _2, and so on. If we expand one of the relu, we'll see:\n\n<img src='https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example01/graph04.PNG' width=600>\n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\" width=\"20\", align=\"left\"></img> &nbsp; Weights and bias are now within the name_scope relu2 so we don't see _2 appended to the name.", 
            "metadata": {}
        }, 
        {
            "cell_type": "markdown", 
            "source": "### Sharing Variables\n\nSuppose you want to control the ReLU threshold (currently hardcoded to 0) using a shared threshold variable for all ReLUs. You can use the *`get_variable()`* function to create the shared variable if it does not exist yet, or reuse it if it already exists. The desired behavior (creating or reusing) is controlled by an attribute of the current *`variable_scope()`*. For example, the following code will create a variable named *`\"relu/threshold\"`* (as a scalar, since *`shape=()`*, and using\n0.0 as the initial value):", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "with tf.variable_scope(\"relu\"):\n    threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "<img src=\"https://github.com/tuliplab/mds/blob/master/Jupyter/image/warning.png\" width=\"40\", align=\"left\"></img> If the variable has already been created by an earlier call to *`get_variable()`*, this code will raise an exception. This behavior prevents reusing variables by mistake. If you want to reuse a variable, you need to explicitly say so by setting the variable scope\u2019s reuse attribute to *`True`*.\n", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "with tf.variable_scope(\"relu\", reuse=True):\n    threshold = tf.get_variable(\"threshold\")", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "This code will fetch the existing *`\"relu/threshold\"`* variable, or raise an exception if it does not exist or if it was not created using *`get_variable()`*. Alternatively, you can set the reuse attribute to *`True`* inside the block by calling the scope\u2019s *`reuse_variables()`* method:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "with tf.variable_scope(\"relu\") as scope:\n    scope.reuse_variables()\n    threshold = tf.get_variable(\"threshold\")", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "Now you have all the pieces you need to make the *`relu()`* function access the threshold variable without having to pass it as a parameter:", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "tf.reset_default_graph()\n\ndef relu(X):\n    with tf.variable_scope('relu', reuse=True):\n        threshold = tf.get_variable(\"threshold\")\n        w_shape = (int(X.get_shape()[1]), 1)\n        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n        b = tf.Variable(0.0, name=\"bias\")\n        z = tf.add(tf.matmul(X, w), b, name=\"linear\")\n        return tf.maximum(z, threshold, name=\"max\")\n\nn_features = 3\nX = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n\nwith tf.variable_scope(\"relu\"): # create the variable\n    threshold = tf.get_variable(\"threshold\", shape=(),\n                                initializer=tf.constant_initializer(0.0))\n    \nrelus = [relu(X) for i in range(5)]\noutput = tf.add_n(relus, name=\"output\")\n\nlogdir = 'tf_logs/example01/customized_relu'\nfile_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\nfile_writer.close()", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "<img src='images/example01/graph05.PNG' width=800>\n\nThis code first defines the *`relu()`* function, then creates the *`relu/threshold`* variable (as a scalar that will later be initialized to 0.0) and builds five ReLUs by calling the *`relu()`* function. The *`relu()`* function reuses the *`relu/threshold`* variable, and creates the other ReLU nodes. Therefore, we see the \"scalar\" connections between the first created ReLU and the other 5 ReLUs, meaning that 5 ReLUs reuse the threshold scalar from the first created ReLU.\n\nTo avoid creating the first unused ReLU. We can pass the reuse argument to the function *`relu()`*. We'll set *`reuse = True`* when after the first ReLU is created.", 
            "metadata": {}
        }, 
        {
            "cell_type": "code", 
            "execution_count": null, 
            "metadata": {}, 
            "source": "tf.reset_default_graph()\n\ndef relu(X, reuse=False):\n    with tf.variable_scope('relu') as scope:\n        if reuse:\n            scope.reuse_variables()\n            \n        threshold = tf.get_variable(\"threshold\", shape=(), initializer=tf.constant_initializer(0.0))\n        \n        w_shape = (int(X.get_shape()[1]), 1)\n        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n        b = tf.Variable(0.0, name=\"bias\")\n        z = tf.add(tf.matmul(X, w), b, name=\"linear\")\n        return tf.maximum(z, threshold, name=\"max\")\n\nn_features = 3\nX = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n\nrelus = [relu(X, reuse=i>0) for i in range(5)]\noutput = tf.add_n(relus, name=\"output\")\n\nlogdir = 'tf_logs/example01/customized_relu2'\nfile_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\nfile_writer.close()", 
            "outputs": []
        }, 
        {
            "cell_type": "markdown", 
            "source": "<img src='https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example01/graph06.PNG' width=600>\n\nThis time, exactly 5 ReLUs are created, with *`ReLU_[1,2,3,4]`* reuse the threshold scalar from the first ReLU.", 
            "metadata": {}
        }
    ], 
    "nbformat": 4, 
    "nbformat_minor": 2, 
    "metadata": {
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 3.5 (Experimental) with Spark 1.6 (Unsupported)", 
            "name": "python3"
        }, 
        "language_info": {
            "pygments_lexer": "ipython3", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "name": "python", 
            "file_extension": ".py", 
            "version": "3.5.2", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }
}