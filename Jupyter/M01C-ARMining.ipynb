{
    "metadata": {
        "language_info": {
            "name": "python", 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 3
            }, 
            "file_extension": ".py", 
            "version": "3.5.2"
        }, 
        "anaconda-cloud": {}, 
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 2.1", 
            "name": "python3-spark21", 
            "language": "python"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "source": "# Modern Data Science \n**(Module 01: A Touch of Data Science)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session C - Association Rule Mining", 
            "cell_type": "markdown"
        }, 
        {
            "metadata": {}, 
            "source": "Apiori Algorithm is a Association Rule learning Algorithm and is used to find frequent itemsets and relevant association rules. By frequent itemsets we should think of items who are frequently associated with each other within the whole dataset. In order for a frequent itemset to be valid the subsets of the itemset needs to be frequent by themselves, this is called anti-monotonicity or downward-closure poperty. The anti-monotonicity guarantees a efficient search, this is necessary because the algorithm goes through all combinations adding one at a time. The process of adding one at a time is called buttom up approach.\n\n## `apyori` Package\n\nIn this part, we will use the `apyori` package. \n\nHere we assume a simple transaction set, and then the calling of `apriori` is straightforward:", 
            "cell_type": "markdown"
        }, 
        {
            "source": "from apyori import apriori\n\ntransactions = [\n    ['beer', 'nuts'],\n    ['beer', 'cheese'],\n]\n\nresults = list(apriori(transactions))", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "print(results)", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "for i in range(len(results)):\n    print(\"##############################################################################\")\n    print(i)\n    print(results[i])\n    print(results[i].items)", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "## Transactions from File\n\nIf the transaction set is stored on an external file, we might need to use some code to load it into the proper format:\n\nConsider a file with the following content:\n\n![DS1](http://franklinwillemen.com/machine-learning/assets/img/ML/Association_Rule_Learning/apriori_dataset1.jpg \"Data Structure\")\n\nWhen importing our dataset `Market_Basket_Optimization`, we should add the `header` argument with the value `None`, without this Python places the first row of the dataset as the titles of the columns and we do not want this. If we take a look at the dataframe now we can see that our product names are now located in the first row. \n\n\nThe next thing is that we will preprocess our data set by placing it in Lists of Lists, we do this using two for loops. The first for loop goes through all the rows (transactions) of the dataset and the second for loop goes through all the products and adds these to a list before appending them to the transactions list. We can see that our dataset is now of the type list and that it's value is indeed a list of products. \n\n![DS2](http://franklinwillemen.com/machine-learning/assets/img/ML/Association_Rule_Learning/apriori_dataset2.jpg \"Data Structure2\")\n\n\nWe will not go into details of this code because there are various ways to get to the same result with Python, just note that the second for loop is actually a inner for loop, nested in the append method. \n\nAlso note that the `dataset.values[i,j]` is converted to a string value when appended to the transaction list.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "import wget\n\nlink_to_data = 'https://github.com/tuliplab/mds/raw/master/Jupyter/data/Market_Basket_Optimisation.csv'\nDataSet = wget.download(link_to_data)", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "import pandas as pd\n\n# Data Preprocessing\ndataset = pd.read_csv('Market_Basket_Optimisation.csv', header = None)\ntransactions = []\nfor i in range(0, 7501):\n    transactions.append([str(dataset.values[i,j]) for j in range(0, 20)])", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "Before we train our apriori algorithm on the dataset, let's import the apyori.py library which contains the classes we will be using. We will declare a variable called rules since the apriori algorithm takes the transactions list as input and outputs association rules. If we inspect our Apiori function we see it accepts one main argument and that is an iterable object our transactions list. We can also specify the support, confidence and lift thresholds. A new argument which we have not discussed yet is min_lenght, it specify's the minimum length of our rules, we will set this to 2 to prevent rules with only one item.\n\nWhen deciding on the minimum support we should calculate this in a way that it makes sense according to the problem at hand. We want to optimize the sales for a supermarket, the dataset contains all the transactions made in one week. If we want to have products who are minimally bought 3 times a day our minimum support will be 0.0028. (3*7/7500)\n\nThe minimum confidence should prevent that rules are made based on the wrong reasons, there are products who are bought frequently together not because they're strongly associated but because they are frequently bought products independent of each other. We want our rules to be relevant, choosing 0.8 is to high and will give us no results on this dataset, 0.4 will give us irrelevant rules as described earlier, the winner is 0.2 because this confidence minimum gave us relevant rules that were proven true enough times but not too frequent to be irrelevant to our goal. This minimum is found by running the Apriori algorithm different times and checking if the results makes sense. \n\nThe lift measurement is a good indication of the relevance of our rules, we will set our minimum lift to 3 because we can be sure that these rules will be significant. Remember that these arguments depends on your dataset or problem. The Apriori is an experimental algorithm, we need to try different threshold and see if the produced rules have the wanted effect in the real world if not than we have to tune our thresholds some more.", 
            "cell_type": "markdown"
        }, 
        {
            "source": "# Training Apriori on the dataset\nfrom apyori import apriori\nrules = apriori(transactions, min_support = 0.003, min_confidence = 0.2, min_lift = 3, min_length = 2)\n\n# Visualising the results\nresults = list(rules)\n\nmyResults = [list(x) for x in results] \n", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "print(myResults)", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "for i in range(len(results)):\n    print(\"##############################################################################\")\n    print(i)\n    print(results[i])\n    print(results[i].items)", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "metadata": {}, 
            "source": "\n## Appendix\n\n#### An Example: finding similar features in poisonous mushrooms", 
            "cell_type": "markdown"
        }, 
        {
            "source": "import wget\n\nlink_to_data = 'https://github.com/tuliplab/mds/raw/master/Jupyter/data/mushroom.dat'\nDataSet = wget.download(link_to_data)", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "mushDatSet = [line.split() for line in open('mushroom.dat').readlines()]", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "L,suppData=apriori(mushDatSet, minSupport=0.3)", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "for item in L[1]:\n    if item.intersection('2'): \n        print (item)", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "source": "for item in L[3]:\n    if item.intersection('2'): \n        print (item)", 
            "metadata": {}, 
            "execution_count": null, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat_minor": 2
}