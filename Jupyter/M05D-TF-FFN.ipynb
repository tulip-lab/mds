{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 05: Deep Learning)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session D - Deep Feedforward Neural Networks\n\n**The purpose of this session is to demonstrate how to use TensorFlow to develop machine learning algorithms and deep neural network models. In this practical session, we present the following topics:**\n\n1. How to implement classification algorithms using Tensorflow\n2. Learning  standard and advanced gradient optimization methods and use these predefined optimizer inTensorFlow\n3. How to build a deep neural networks for images classification problems usingTensorFlow\n\n** References and additional reading and resources**\n- [An Introduction to Implementing Neural Networks using TensorFlow](https://www.analyticsvidhya.com/blog/2016/10/an-introduction-to-implementing-neural-networks-using-tensorflow/)\n- [Tensorflow Tutorials](https://www.tensorflow.org/tutorials/)\n- [Deep Learning Book](http://www.deeplearningbook.org/)\n- [37 Reasons why your Neural Network is not working](https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607)\n\n---\n\n\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## <span style=\"color:#0b486b\">1. Classification with TensorFlow</span>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "For a classification problem, we wish to predict disrcrete label $y$ given features $\\mathbf{x}=\\left[x_{1},x_{2},...,x_{N}\\right]^{T}$, wherein $y\\in\\left\\{0,1\\right\\}$ for binary classification and $y\\in\\left\\{ 1,...,K\\right\\}$ for multiclass classification. We typically solve this problem by learning a function to predict log-probability that an example belong to each class and then apply the principle of maximum likelihood to derive the loss function. Let's first consider the binary case. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### <span style=\"color:#0b486b\">1.1. Binary classification</span>\n#### Problem formulation\nFor the binary case, we need to learn the class-conditional densities:  \n\n$$\np(y=1 \\mid \\mathbf{x})\t=\t\\frac{p(\\mathbf{x} \\mid y=1)p(y=1)}{p(\\mathbf{x} \\mid y=0)p(y=0)+p(\\mathbf{x} \\mid y=1)p(y=1)}\n\t=\t\\frac{1}{1+\\frac{p(\\mathbf{x} \\mid y=0)p(y=0)}{p(\\mathbf{x} \\mid y=1)p(y=1)}}\n\t=\t\\frac{1}{1+\\exp(-a)}\n\t=\t\\sigma(a)\n$$\n\nwhere $a=\\frac{p(\\mathbf{x} \\mid y=1)p(y=1)}{p(\\mathbf{x} \\mid y=0)p(y=0)}$, and $\\sigma(a)$ is the **logistic sigmoid function** defined by:  \n\n$$\n\\sigma(a)=\\frac{1}{1+\\exp(-a)}\n$$ \n\nIf we use a function $f$, which can be modeled using simple linear regression or a neural network with parameter $\\mathbf{\\theta}$, to estimate $a$, we'll have:  \n\n$$\np(y=1 \\mid \\mathbf{x})=\\sigma(a)=\\sigma(f_{\\mathbf{\\theta}}(\\mathbf{x}))\n$$ \n\nFor a set of $M$ training examples with binary labels $\\left\\{(\\mathbf{x}^{(i)},y^{(i)}):i=1,...,M\\right\\}$, the probability according to our model will be:\n\n$$\n\\mathcal{L}=\\prod_{i=1}^{M}p(y=y^{(i)} \\mid \\mathbf{x}^{(i)})=\\prod_{i=1}{M}\\sigma(a_{i})^{y^{(i)}}(1-\\sigma(a_{i}))^{(1-y^{(i)})}\n$$ \n\nwherein $a_{i}=f_{\\mathbf{\\theta}}(\\mathbf{x})$.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### <span style=\"color:#0b486b\">Loss function:</span>\n\nWe need to find $\\mathbf{\\theta}$ that maximizes the likelihood $\\mathcal{L}$. This can be done more easily by minimizing the negative log-likelihood. Therefore, we have the cost function:\n\n$$\nJ(\\mathbf{\\theta})=\\sum_{i=1}^{M}y^{(i)}\\log\\sigma(a_{i})+(1-y^{(i)})\\log(1-\\sigma(a_{i}))\n$$\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### Implementation with TensorFlow\n\nIn terms of implementation, given a scalar $a$ in the case of logistic classification , we can use the following code to calculate the conditional probability:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[0.9999546  0.11920292]\n"
                }
            ], 
            "source": "import tensorflow as tf\na = tf.constant([10.0, -2.0])\ny_proba = tf.nn.sigmoid(a)\nwith tf.Session() as sess:\n    print(y_proba.eval())"
        }, 
        {
            "source": "Then we can calculate the loss function:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "0.06348671\n"
                }
            ], 
            "source": "y = tf.constant([1.0, 0.0])\nloss = tf.reduce_mean(-y * tf.log(y_proba) - (1 - y) * tf.log(1 - y_proba))\nwith tf.Session() as sess:\n    print(loss.eval())"
        }, 
        {
            "source": "However, calculating the probability is numerically unstable when $a$ is too large or too small due to the exponential operation. Try this:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[1.         0.00669285]\nnan\n"
                }
            ], 
            "source": "a = tf.constant([20.0, -5.0])\ny_proba = tf.nn.sigmoid(a)\nwith tf.Session() as sess:\n    print(y_proba.eval())\ny = tf.constant([1.0, 0.0])\nloss = tf.reduce_mean(-y * tf.log(y_proba) - (1 - y) * tf.log(1 - y_proba))\nwith tf.Session() as sess:\n    print(loss.eval())"
        }, 
        {
            "source": "We are thus recommended to estimate the loss directly from logits, using the following code:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "0.0033576752\n"
                }
            ], 
            "source": "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=a))\nwith tf.Session() as sess:\n    print(loss.eval())"
        }, 
        {
            "source": "### <span style=\"color:#0b486b\">1.2. Multiclass classification</span>\n\nIn multiclass classification problem when $K>2$, we have:\n\n$$\np(y=k \\mid \\mathbf{x}) = \\frac{p(\\mathbf{x} \\mid y=k)p(y=k)}{\\sum_{j=1}^{K}p(\\mathbf{x} \\mid y=j)p(y=j)}\n\t= \\frac{\\exp(a_{k})}{\\sum_{j=1}^{K}\\exp(a_{j})}\n\t= \\sigma(\\mathbf{a})_{k}\n$$ \n\nwhere $a_k=\\log p(\\mathbf{x} \\mid y=k)p(y=k)$ is called logits, and $\\sigma(\\mathbf{a})_{k}$ is the softmax function defined by:\n\n$$\n\\sigma(\\mathbf{a})_{k}=\\frac{\\exp(a_{k})}{\\sum_{j=1}^{K}\\exp(a_{j})}\n$$\n\nSimilary to the binary classification, we can use simple linear regression or a neural network with parameter $\\mathbf{\\theta}$   to model a function $f_{\\mathbf{\\theta}}(\\mathbf{x})=\\left[a_{1},a_{2},...,a_{K}\\right]^{T}$. For a set of $M$ training examples with multinominal labels $\\left\\{(\\mathbf{x}^{(i)},y^{(i)}):i=1,...,M\\right\\}$, the probability according to our model will be:\n\n$$\n\\mathcal{L}=\\prod_{i=1}^{M}p(y=y^{(i)}|\\mathbf{x}^{(i)})=\\prod_{i=1}^{M}\\sigma(\\mathbf{a}^{(i)})_{y^{(i)}}\n$$", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### <span style=\"color:#0b486b\">Loss function:</span>\n\nWe need to find $\\mathbf{\\theta}$ that minimizes the negative log-likelihood so, we have the cost function:\n\n$$\nJ(\\mathbf{\\theta})=-\\sum_{i=1}^{M}\\log\\sigma(\\mathbf{a}^{(i)})_{y^{(i)}}\n$$\nIf we express the label for the $i$-th example as a **one-hot** vector $\\mathbf{y}^{(i)}=\\left[y_{1}^{(i)},y_{2}^{(i)},...,y_{K}^{(i)}\\right]^{T}$, where $y_{k}^{(i)}=1$ if the example belongs to class $k$, and $y_{k}^{(i)}=0$ otherwise, the cost function can be rewritten as:   \n\n$$J(\\mathbf{\\theta})=-\\sum_{i=1}^{M}\\sum_{j=1}^{K}\\log y_{j}^{(i)}\\sigma(\\mathbf{a}^{(i)})_{j}$$\n\nWe now can use the following code to calculate the conditional probability:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[[9.9999380e-01 6.1441742e-06]\n [3.3535014e-04 9.9966466e-01]]\n"
                }
            ], 
            "source": "import tensorflow as tf\na = tf.constant([[10.0, -2.0], [-5.0, 3.0]])\ny_proba = tf.nn.softmax(a)\nwith tf.Session() as sess:\n    print(y_proba.eval())"
        }, 
        {
            "source": "Then we can calculate the loss function:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "0.00017079544\n"
                }
            ], 
            "source": "y = tf.constant([[1.0, 0.0], [0.0, 1.0]])\nloss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_proba), axis=1))\nwith tf.Session() as sess:\n    print(loss.eval())"
        }, 
        {
            "source": "However, similar to the problem in binary classification, calculating the probability is numerically unstable when $a$ is too large or too small due to the exponential operation. Try this:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "[[1.0000000e+00 1.3887944e-11]\n [3.3535014e-04 9.9966466e-01]]\nnan\n"
                }
            ], 
            "source": "a = tf.constant([[20.0, -5.0], [-5.0, 3.0]])\ny_proba = tf.nn.softmax(a)\nwith tf.Session() as sess:\n    print(y_proba.eval())\ny = tf.constant([[1.0, 0.0], [0.0, 1.0]])\nloss = tf.reduce_mean(-y * tf.log(y_proba) - (1 - y) * tf.log(1 - y_proba))\nwith tf.Session() as sess:\n    print(loss.eval())"
        }, 
        {
            "source": "We are thus recommended to estimate the loss directly from logits, using the following code:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "0.00016769934\n"
                }
            ], 
            "source": "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=a)) \nwith tf.Session() as sess:\n    print(loss.eval())"
        }, 
        {
            "source": "When labels is a one dimensional vector, we use the following code:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "10.000171\n"
                }
            ], 
            "source": "a = tf.constant([[10.0, -2.0], [-5.0, 3.0]])\ny = tf.constant([1, 0])\nloss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=a))\nwith tf.Session() as sess:\n    print(loss.eval())"
        }, 
        {
            "source": "### <span style=\"color:#0b486b\">1.3.3. Gradient learning</span>\n\nOnce you define the loss function, you can minimize the loss by using gradient descent algorithm or its variants. The idea is that starting at a point in the parameter space, following the directions of gradients estimated at that point will increase the loss function. To reduce the loss, you should go the opposite directions. So, the basic gradient descent algorithm is to iteratively update the parameters until reaching a global (local) minima. For parameters $\\mathbf{\\theta}$ with gradients $\\frac{\\partial{J}}{\\partial{\\mathbf{\\theta}}}$, the update will take the form:\n\n$$\n\\mathbf{\\theta}=\\mathbf{\\theta}-\\eta\\frac{\\partial{J}}{\\partial{\\mathbf{\\theta}}}\n$$\n\nwhere $\\eta > 0$ is the learning rate. Learning rate $\\eta$ is an important parameter to be tuned. A low learning rate cause the model to learn slowly. High learning rate can help the model learn faster but may fail to converge as the parameters will bound around the convergence point in the parameter space.\n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example02/high-learning-rate.png\", width=300>\n\nA simple way to select learning rate is to try different learning rates, typically between `1e-5` and `1`, and draw the graph of loss after each iterations or epochs. The following image sketches some scenerios of good or bad learning rate:\n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example02/learning-rate.jpg\", width=300>\n\nThere are variants of gradient descent that can reach convergence faster but they are all based on gradients. In what follows, we'll briefly talk about some of them.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### <span style=\"color:#0b486b\">1.3.1. Stochastic Gradient Descent (SGD)</span>\n\nFor gradient descent, we need to empirically estimate the average loss using all training data:\n$$\nJ(\\mathbf{\\theta})=\\frac{1}{M}\\sum_{i=1}^{M}\\mathcal{L}(\\mathbf{x}^{(i)}, y^{(i)}, \\mathbf{\\theta})\n$$\n\nwhere $\\mathcal{L}$ is the per-example loss $\\mathcal{L}(\\mathbf{x}, y, \\mathbf{\\theta})=-\\log{p(y|\\mathbf{x};\\mathbf{\\theta})}$.\nUnfortunately, estimating the average loss over a large training set is computationally expensive. Stochastic gradient descent (**SGD**) is an extension of the gradient descent algorithm. For each step of the training algorithm, we can sample a minibatch of example $\\mathbb{B}=\\{\\mathbf{x}^{(1)},\\mathbf{x}^{(2)},..,\\mathbf{x}^{(m^{\\prime})}\\}$ The minibatch size $m^{\\prime}$ is typically chosen to be a relatively small number of examples, ranging from 1 to a few hundred. The estimate of the gradient will be:\n$$\n\\mathbf{g}=\\frac{1}{m^{\\prime}}\\nabla_{\\mathbf{\\theta}}\\sum_{i=1}^{m^{\\prime}}\\mathcal{L}(\\mathbf{x}^{(i)}, y^{(i)}, \\mathbf{\\theta})\n$$\n\nThe weight update in stochastic gradient descent will be:\n\n$$\n\\mathbf{\\theta}=\\mathbf{\\theta}-\\eta\\mathbf{g}\n$$\n\nOptimization algorithms that use only a single example at a time are called stochastic methods while optimization algorithms that use more than a single examples at a time are traditionally called minibatch stochastic methods. It's now common to simply call them stochastic methods.\n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\" width=\"20\", align=\"left\"></img> **Some notes**:\n- Using small batch size can lead to unstable estimates. The standard error of the mean estimated from $m^{\\prime}$ samples is given by $\\sigma\\mathbin{/}\\sqrt{m^{\\prime}}$. So, Training with such a small batch size might require a small learning rate to maintain stability due to the high variance in the estimate of the gradient.\n- Large batches provide a more accurate estimate of the gradient. However, as we increase batch size from 100 to 10,000, the latter requires 100 times more computation than the former but reduces the standard error of the mean only by a factor of 10.\n- Small batches can offer regularizing effect perhaps due to the noise they add to during the learning process. Generalization error is often best for a batch size of 1. However, the total runtime can be very high due to the need to make more steps, both because of the reduced learning rate and because it takes more steps to observe the entire training set.\n- Multicore architectures are usually underutilized by extremely small batches.\n- Some kinds of hardware achieve better runtime with specific sizes of arrays. Especially when using GPUs, it is common for power of 2 batch sizes to offer better runtime. Typical power of 2 batch sizes range from 32 to 256, with 16 sometimes being attempted for large models.\n- The simplest way to do feed a minibatch to our computational graph in Tensorflow is to use placeholder nodes. They are typically used to pass the training data to TensorFlow during training.\n\n**Reference: Chapter 8 - Adaptive Computation and Machine Learning, Deep Learning Textbook**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### <span style=\"color:#0b486b\">1.3.2. Momentum</span>\n\nStochastic gradient descent can sometimes be slow, especially when the gradients are of high curvature, small but consistent or noisy. Imagine the loss as the height of a canyon with steep sides. Randomly initializing the parameters is like starting at a location in the canyon and optimizing the loss is like going down the canyon. The black path in the picture below depict how SGD wastes time moving back and forth across the narrow axis of the canyon:\n\n<figure>\n  <img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example02/momentum.PNG\" width=300>\n  <figcaption text-align=\"center\">(*The picture is adapted from Chapter 8, Deep Learning Textbook*)</figcaption>\n</figure>\n\nMomentum algorithm, proposed by Boris Polyak in 1964, accelerates learning in such situation by accumulating an exponentially decaying moving average of past gradients and continues to move in their direction. Momentum introduces a velocity vector $\\mathbf{v}$. It is the direction and the speed at which the parameters move through the hyperparameter space. The velocity is set to an exponentially decaying average of the negative gradient. We can interpret the decay as a result of friction that keeps the velocity from growing too large. The name momentum is derived from a physic analogy, in which the negative gradient is a force moving a particle through parameter space, according to Newton\u2019s laws of motion. Momentum in physics is mass times velocity. In the momentum learning algorithm, we assume unit mass, so the velocity vector $\\mathbf{v}$ may also be regarded as the momentum $\\mathbf{m}$ of the particle.\n\nAt each iteration, the local gradient (multiplied by the learning rate $\\eta$) is added to the momentum vector $\\mathbf{m}$, and you can update the weights by simply subtract this momentum vector:\n\n$$\n\\begin{align}\n\\mathbf{m} &= \\beta\\mathbf{m} + \\eta\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}) \\\\\n\\mathbf{\\theta} &= \\mathbf{\\theta} - \\mathbf{m}\n\\end{align}\n$$\n\nwhere $\\beta$ is the momentum hyperparameter. The higher the hyperparameter is, the bigger the influence of past gradient is. The momentum hyperparameter is usually start at a small number, such as 0.5, and increased gradually to 0.9 or 0.99.\n\nWhen local gradient keeps pointing to a certain direction, momentum will pickup in that direction, allowing the update path to tranverse the canyon lengthwise and converge faster, as shown by the red path in the picture above.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### <span style=\"color:#0b486b\">1.3.3. AdaGrad</span>\n\nConsider a elongated bowl with a gentle slope, gradient descent starts by starts by quickly going down the steepest slope, then slowly goes down the bottom of the valley. We may wish a greater progress in the more gently slopped directions of the hyperparmeter space.\n\n<img src='https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/dl/example02/adagrad.PNG' width=500>\n\n(*The picture is adapted from chapter 11, Hands-On Machine Learning with Scikit-Learn and Tensorflow*)\n\nAdaGrad achieves this goal by individually adapts the learning rates of all model parameters by scaling them inversely proportional to the square root of the sum of all of their historical squared values. The parameters with the largest partial derivative of the loss have a correspondingly rapid decrease in their learning rate, while parameters with small partial derivatives have a relatively small decrease in their learning rate.\n\nEach update in the AdaGrad algorithm takes two steps. The first step in AdaGrad algorithm is to accumulate the square of the gradients into the vector $\\mathbf{s}$. The second step is to update parameters as usual, however with one big difference: the learning rate is scaled down by a factor of $\\sqrt{\\mathbf{s}+\\epsilon}$ where $\\epsilon$ is a small number, typically 1e-8, to avoid division by zero.\n\n$$\n\\begin{align}\n\\mathbf{s} &= \\mathbf{s}+\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})\\otimes{J(\\mathbf{\\theta})} \\\\\n\\mathbf{\\theta} &= \\mathbf{\\theta}-\\eta\\nabla_{\\theta}J(\\mathbf{\\theta})\\oslash\\sqrt{\\mathbf{s}+\\epsilon}\n\\end{align}\n$$\n\nwhere $\\otimes$ is element-wise product and $\\oslash$ is element-wise division.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### <span style=\"color:#0b486b\">1.3.4. RMSProp</span>\n\nEmpirically AdaGrad has been found that\u2014for training deep neural network models\u2014the accumulation of squared gradients from the beginning of training can result in a premature and excessive decrease in the effective learning rate. AdaGrad performs well for some but not all deep learning models. AdaGrad is designed to converge rapidly when applied to a convex function. When applied to a non-convex function to train a neural network, the learning trajectory may pass through many different structures and eventually arrive at a region that is a locally convex bowl AdaGrad shrinks the learning rate according to the entire history of the squared gradient and may have made the learning rate too small before arriving at such a convex structure.\n\nThe RMSProp algorithm modifies AdaGrad by changing the gradient accumulation in the first step into an exponentially weighted moving average:\n\n$$\n\\begin{align}\n\\mathbf{s} &= \\beta\\mathbf{s}+(1-\\beta)\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})\\otimes{J(\\mathbf{\\theta})} \\\\\n\\mathbf{\\theta} &= \\mathbf{\\theta}-\\eta\\nabla_{\\theta}J(\\mathbf{\\theta})\\oslash\\sqrt{\\mathbf{s}+\\epsilon}\n\\end{align}\n$$\n\nUsing exponentially decaying average allows RMSProp to discard history from the extreme past so that it can converge rapidly after finding a convex bowl.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### <span style=\"color:#0b486b\">1.3.5. Adam</span>\n\n**Adam** combines the ideas of Momentum optimization and RMSProp: just like Momentum optimization it keeps track of an exponentially decaying average of past gradients, and just like RMSProp it keeps track of an exponentially decaying average of past squared\ngradients.\n\nThere are five steps in Adam:\n\n$$\n\\begin{align}\n\\mathbf{m} &= \\beta_{1}\\mathbf{m}+(1-\\beta_{1})\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta}) \\\\\n\\mathbf{s} &= \\beta_{2}\\mathbf{s}+(1-\\beta_{2})\\nabla_{\\mathbf{\\theta}}J(\\mathbf{\\theta})\\otimes{J(\\mathbf{\\theta})} \\\\\n\\mathbf{m} &= \\frac{\\mathbf{m}}{1-\\beta_{1}^{t}} \\\\\n\\mathbf{s} &= \\frac{\\mathbf{s}}{1-\\beta_{2}^{t}} \\\\\n\\mathbf{\\theta} &= \\mathbf{\\theta}-\\eta\\mathbf{m}\\oslash\\sqrt{\\mathbf{s}+\\epsilon} \\\\\n\\end{align}\n$$\n\nwhere $t$ is the iterationi number, starting at 1.\n\nStep 1 and 2 estimate the first and second order moment of the gradient. Steps 3 and 4 are somewhat of a technical detail: since $m$ and $s$ are initialized at 0, they will be biased toward 0 at the beginning of training, so these two steps will help boost $m$ and $s$ at the beginning of training.\n\nThe momentum decay hyperparameter $\\beta_{1}$ is typically initialized to 0.9, while the scaling decay hyperparameter $\\beta_{2}$ is often initialized to 0.999. As earlier, the smoothing term $\\epsilon$ is usually initialized to a tiny number such as $10^{-8}$.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "#### <span style=\"color:#0b486b\">1.3.6. Implementation in TensorFlow</span>\n\nTo implement gradient descent-based optimization, you'll need to calculate the gradients of the loss. You can manually derive the gradients from the cost function. In the case of Linear Regression, it is reasonably easy, but if you had to do this with deep neural networks we would get quite a headache: it would be tedious and error-prone. You can instead use TensorFlow\u2019s autodiff feature to let TensorFlow compute the gradients automatically or use a couple of TensorFlow\u2019s out-of-the-box optimizers.\n\n<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/note.gif\" width=\"60\", align=\"left\"></img> When using Gradient Descent, remember that it is important to first normalize the input feature vectors, or else training may be much slower. You can do this using TensorFlow, NumPy, Scikit-Learn\u2019s StandardScaler, or any other solution you prefer. The\nfollowing code assumes that this normalization has already been done.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "TensorFlow also provides a number of **off-the-shelf optimizers**, including a Gradient Descent optimizer. You can simply declare an optimizer and add an *`op`* that performs an upgrade step:\n\n``optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\ntraining_op = optimizer.minimize(mse)``", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Current list of optimizers implemented in TensorFlow is:\n1. tf.train.GradientDescentOptimizer\n* tf.train.AdadeltaOptimizer\n* tf.train.AdagradOptimizer\n* tf.train.AdagradDAOptimizer\n* tf.train.MomentumOptimizer\n* tf.train.AdamOptimizer\n* tf.train.FtrlOptimizer\n* tf.train.ProximalGradientDescentOptimizer\n* tf.train.ProximalAdagradOptimizer\n* tf.train.RMSPropOptimizer\n\n... and more are coming. ***Reference***: [https://www.tensorflow.org/api_guides/python/train](https://www.tensorflow.org/api_guides/python/train)\n\nIf you want to try momentum optimizer or Adam optimizer, just replace with one of the following lines of code:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "learning_rate=0.1\noptimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9)"
        }, 
        {
            "source": "### <span style=\"color:#0b486b\">1.4. Putting it altogether for multiclass classification</span>\n\nNow, we've had all ingredients to build a logistic or softmax classification. Let's build a simple softmax classification with MNIST dataset.\n\n* #### <span style=\"color:#0b486b\">Step 1: Load or download the dataset</span>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "apache_access_log-beaa3.PROJECT  MNIST_data\r\nbrain_body_data.csv\t\t models\r\ncorrected.gz\t\t\t mtcars.csv\r\ndac_sample.txt\t\t\t pendigits.txt\r\ndatasets\t\t\t PierceCricketData.csv\r\ndl\t\t\t\t poker.txt\r\nkddcup.data_10_percent.gz\t sample_svm_data.txt\r\nkddcup.data.gz\t\t\t summary_logs\r\nmillionsong.txt\t\t\t tf_logs\r\n"
                }
            ], 
            "source": "!ls\n#! mkdir dl"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n"
                }, 
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Extracting dl/train-images-idx3-ubyte.gz\nExtracting dl/train-labels-idx1-ubyte.gz\nExtracting dl/t10k-images-idx3-ubyte.gz\nExtracting dl/t10k-labels-idx1-ubyte.gz\n"
                }
            ], 
            "source": "from tensorflow.examples.tutorials.mnist import input_data\n\n# https://github.com/tuliplab/mds/tree/master/Jupyter/data/dl\n# You may need to download all data to a local folder\n\nmnist = input_data.read_data_sets(\"dl/\")\nX_train = mnist.train.images\nX_test = mnist.test.images\ny_train = mnist.train.labels.astype(\"int\")\ny_test = mnist.test.labels.astype(\"int\")"
        }, 
        {
            "source": "* #### <span style=\"color:#0b486b\">Step 2: Build the graph using TensorFlow</span>\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import tensorflow as tf\ntf.reset_default_graph()\n\nn_inputs = 28 * 28\nn_hidden1 = 300\nn_hidden2 = 100\nn_outputs = 10\nlearning_rate = 0.01\n\nX = tf.placeholder(tf.float32, shape=[None, n_inputs], name='X')\ny = tf.placeholder(tf.int64, shape=[None], name='y')\n\nW = tf.Variable(tf.truncated_normal([n_inputs, n_outputs], stddev=0.02), name='weights')\nb = tf.Variable(tf.zeros([n_outputs]), name='biases')\n\nlogits = tf.add(tf.matmul(X, W), b, name='logits')\n\nwith tf.name_scope('evaluation'):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits, name='xentropy')\n    loss = tf.reduce_mean(xentropy, name='loss')\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n\nwith tf.name_scope(\"train\"):\n    grad_W, grad_b = tf.gradients(loss, [W, b])\n    update_W = tf.assign(W, W - learning_rate * grad_W)\n    update_b = tf.assign(b, b - learning_rate * grad_b)\n\ninit = tf.global_variables_initializer()"
        }, 
        {
            "source": "* #### <span style=\"color:#0b486b\">Step 3: Train the model</span>\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Epoch\tTrain accuracy\tTest accuracy\n0\t0.860000\t0.873100\n1\t0.840000\t0.885600\n2\t0.840000\t0.893900\n3\t0.840000\t0.898700\n4\t0.800000\t0.902700\n5\t0.920000\t0.905500\n6\t0.980000\t0.907900\n7\t0.900000\t0.909200\n8\t0.820000\t0.910200\n9\t0.880000\t0.911600\n10\t0.840000\t0.912300\n11\t0.900000\t0.913600\n12\t0.940000\t0.914100\n13\t0.860000\t0.914000\n14\t0.920000\t0.914200\n15\t0.920000\t0.915700\n16\t0.880000\t0.916400\n17\t0.900000\t0.916700\n18\t0.980000\t0.917000\n19\t0.940000\t0.916900\n"
                }
            ], 
            "source": "n_epochs = 20\nbatch_size = 50\n\nwith tf.Session() as sess:\n    init.run()\n    print(\"Epoch\\tTrain accuracy\\tTest accuracy\")\n    for epoch in range(n_epochs):\n        for iteration in range(mnist.train.num_examples // batch_size):\n            X_batch, y_batch = mnist.train.next_batch(batch_size)\n            sess.run([update_W, update_b], feed_dict={X: X_batch, y: y_batch})\n        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n        print('%d\\t%f\\t%f' % (epoch, acc_train, acc_test))"
        }, 
        {
            "source": "We end up with the testing acurracy of about 92% that is not bad result. Now let's try using black-box gradient descent optimizer of TensorFlow. We will change the code in **Step 2**, which we denote as **Step 2(a)**, **Step 2(b)**,... The code for **Step 3** remains the same the we call **Step 3 (replicate)**.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "* #### <span style=\"color:#0b486b\">Step 2(a): Build the graph using TensorFlow gradient descent optimizer</span>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import tensorflow as tf\ntf.reset_default_graph()\n\nn_inputs = 28 * 28\nn_hidden1 = 300\nn_hidden2 = 100\nn_outputs = 10\nlearning_rate = 0.01\n\nX = tf.placeholder(tf.float32, shape=[None, n_inputs], name='X')\ny = tf.placeholder(tf.int64, shape=[None], name='y')\n\nW = tf.Variable(tf.truncated_normal([n_inputs, n_outputs], stddev=0.02), name='weights')\nb = tf.Variable(tf.zeros([n_outputs]), name='biases')\n\nlogits = tf.add(tf.matmul(X, W), b, name='logits')\n\nwith tf.name_scope('evaluation'):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits, name='xentropy')\n    loss = tf.reduce_mean(xentropy, name='loss')\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n\nwith tf.name_scope(\"train\"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    training_op = optimizer.minimize(loss)\n\ninit = tf.global_variables_initializer()"
        }, 
        {
            "source": "* #### <span style=\"color:#0b486b\">Step 3 (replicate): Train the model</span>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Epoch\tTrain accuracy\tTest accuracy\n0\t0.860000\t0.871500\n1\t0.940000\t0.887900\n2\t0.940000\t0.894100\n3\t0.900000\t0.899800\n4\t0.920000\t0.903400\n5\t0.880000\t0.906500\n6\t0.920000\t0.907100\n7\t0.920000\t0.909200\n8\t0.960000\t0.911200\n9\t0.900000\t0.912100\n10\t0.900000\t0.912800\n11\t0.960000\t0.914000\n12\t0.940000\t0.914200\n13\t0.880000\t0.914700\n14\t0.940000\t0.914800\n15\t0.860000\t0.916500\n16\t0.940000\t0.916700\n17\t0.940000\t0.916700\n18\t0.920000\t0.917000\n19\t0.920000\t0.916900\n"
                }
            ], 
            "source": "n_epochs = 20\nbatch_size = 50\n\nwith tf.Session() as sess:\n    init.run()\n    print(\"Epoch\\tTrain accuracy\\tTest accuracy\")\n    for epoch in range(n_epochs):\n        for iteration in range(mnist.train.num_examples // batch_size):\n            X_batch, y_batch = mnist.train.next_batch(batch_size)\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n        print('%d\\t%f\\t%f' % (epoch, acc_train, acc_test))"
        }, 
        {
            "source": "We end up with the same result as of using auto-gradient, which is expected. Now let's try **Adam optimizer**.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "* #### <span style=\"color:#0b486b\">Step 2(b): Build the graph using TensorFlow Adam optimizer</span>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import tensorflow as tf\ntf.reset_default_graph()\n\nn_inputs = 28 * 28\nn_hidden1 = 300\nn_hidden2 = 100\nn_outputs = 10\nlearning_rate = 0.001\n\nX = tf.placeholder(tf.float32, shape=[None, n_inputs], name='X')\ny = tf.placeholder(tf.int64, shape=[None], name='y')\n\nW = tf.Variable(tf.truncated_normal([n_inputs, n_outputs], stddev=0.02), name='weights')\nb = tf.Variable(tf.zeros([n_outputs]), name='biases')\n\nlogits = tf.add(tf.matmul(X, W), b, name='logits')\n\nwith tf.name_scope('evaluation'):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits, name='xentropy')\n    loss = tf.reduce_mean(xentropy, name='loss')\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n\nwith tf.name_scope(\"train\"):\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n    training_op = optimizer.minimize(loss)\n\ninit = tf.global_variables_initializer()"
        }, 
        {
            "source": "* #### <span style=\"color:#0b486b\">Step 3 (replicate): Train the model</span>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Epoch\tTrain accuracy\tTest accuracy\n0\t0.900000\t0.909800\n1\t0.940000\t0.918700\n2\t0.920000\t0.922700\n3\t0.940000\t0.924300\n4\t0.960000\t0.924000\n5\t0.980000\t0.925500\n6\t0.880000\t0.927600\n7\t0.960000\t0.927400\n8\t0.900000\t0.925600\n9\t0.920000\t0.927800\n10\t0.940000\t0.927800\n11\t0.980000\t0.924300\n12\t0.920000\t0.928100\n13\t0.900000\t0.925300\n14\t0.900000\t0.928500\n15\t0.980000\t0.928800\n16\t0.920000\t0.927900\n17\t0.940000\t0.929600\n18\t0.940000\t0.928200\n19\t0.940000\t0.928000\n"
                }
            ], 
            "source": "n_epochs = 20\nbatch_size = 50\n\nwith tf.Session() as sess:\n    init.run()\n    print(\"Epoch\\tTrain accuracy\\tTest accuracy\")\n    for epoch in range(n_epochs):\n        for iteration in range(mnist.train.num_examples // batch_size):\n            X_batch, y_batch = mnist.train.next_batch(batch_size)\n            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n        print('%d\\t%f\\t%f' % (epoch, acc_train, acc_test))"
        }, 
        {
            "source": "We have just improved the accuracy by 1%, which is not really cool but promising.\n\nNow we are going to use deeper and more powerful models in the consequence section.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 1.4  Exercises", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Train classifier in Section A.3 the using some other optimizers provided by TensorFlow: [Adadelta](https://www.tensorflow.org/api_docs/python/tf/train/AdadeltaOptimizer) and [RMSProp](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer). You can try with different parameters of these optimizers and report the best values (of parameters and corresponding performances).", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## <span style=\"color:#0b486b\">2. Deep Feedforward Neural Networks</span>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Deep feedforward neural network (DNN) basically is an advanced version of the multilayer perceptron (MLP) with nonlinear hidden activations. DNNs are at the very core of *Deep Learning*. They are versatile, powerful, and scalable, making them\nideal for tackling large and highly complex Machine Learning tasks, such as classifying billions of images\n(e.g., Google Images), powering speech recognition services (e.g., Apple\u2019s Siri), recommending the best\nvideos to watch to hundreds of millions of users everyday (e.g., YouTube), or learning to beat the world\nchampion at the game of Go by examining millions of past games and then playing against itself\n(DeepMind\u2019s AlphaGo).<br>\n\nIn this session, we are going to use TensorFlow's Python API to implement MNIST digit classification problem. We will use minibatch gradient descent to train our network. Generally, the first step is the construction phase, i.e., building the TensorFlow graph, and the second step is the execution phase, where you actually run the graph to train the model. \n\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Now let's first import the TensorFlow library and load the MNIST dataset.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Extracting dl/train-images-idx3-ubyte.gz\nExtracting dl/train-labels-idx1-ubyte.gz\nExtracting dl/t10k-images-idx3-ubyte.gz\nExtracting dl/t10k-labels-idx1-ubyte.gz\n"
                }
            ], 
            "source": "import tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"dl/\")\nx_train = mnist.train.images\nx_test = mnist.test.images\ny_train = mnist.train.labels.astype(\"int\")\ny_test = mnist.test.labels.astype(\"int\")"
        }, 
        {
            "source": "When developing an application using an DNN, we usually following two main stages:\n\n1. Construction phase\n2. Execution phase (training and testing)\n3. Regularization\n\nHowever, DNNs are easy to be overfitting and producing low performances on testing datasets. Regularization techniques are often used in DNNs to prevent them from overfitting. We also introduce some regularization methods which are popularly used in DNNs  then.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## <span style=\"color:#0b486b\">2.1. Construction phase</span>\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Let\u2019s start. First, we need to specify the number of inputs and outputs, and set the number of hidden neurons in each layer:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "num_inputs = 28 * 28 # this is the size of images in pixels \nnum_hidden1 = 300\nnum_hidden2 = 100\nnum_outputs = 10  # this is the number of classes (label)"
        }, 
        {
            "source": "Next we use placeholder nodes to represent the training data and labels. The shapes of *`x`* and *`y`* are only partially defined to be able to take an arbitrary minibatch size.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "x = tf.placeholder(tf.float32, shape=[None, num_inputs], name=\"x\")\ny = tf.placeholder(tf.int32, shape=[None], name=\"y\")"
        }, 
        {
            "source": "Now we define a function, i.e., layer creator, that can help to create a layer in our multiple layer network.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 25, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def neuron_layer(x, num_neurons, name, activation=None):\n    with tf.name_scope(name):\n        num_inputs = int(x.get_shape()[1])\n        stddev = 2 / np.sqrt(num_inputs)\n        init = tf.truncated_normal([num_inputs, num_neurons], stddev=stddev)\n        W = tf.Variable(init, name=\"weights\")\n        b = tf.Variable(tf.zeros([num_neurons]), name=\"biases\")\n        z = tf.matmul(x, W) + b\n    if activation == \"relu\":\n        return tf.nn.relu(z)\n    else:\n        return z"
        }, 
        {
            "source": "Let's go through the above code line by line:\n\n1. First we create a name scope using the name of the layer: it will contain all the computation nodes for this neuron layer. This is optional, but the graph will look much nicer in TensorBoard if its nodes are well organized.\n2. Next, we get the number of inputs by looking up the input matrix\u2019s shape and getting the size of the second dimension (the first dimension is for instances).\n3. The next three lines create a W variable that will hold the weights matrix. It will be a 2D tensor containing all the connection weights between each input and each neuron; hence, its shape will be `(n_inputs, n_neurons)`. It will be initialized randomly, using a [truncated normal (Gaussian) distribution](https://www.tensorflow.org/api_docs/python/tf/truncated_normal) with a standard deviation of $\\frac{2}{\\sqrt{n_{inputs}}}$. Using this specific standard deviation helps the algorithm converge much faster.\n4. The next line creates $\\mathbf{b}$ variable for biases, initialized to `0` (no symmetry issue in this case), with one bias parameter per neuron.\n5. Then we create a subgraph to compute $\\mathbf{Z} = \\mathbf{W}^{T} \\mathbf{X}  + \\mathbf{b}$. This vectorized implementation will efficiently compute the weighted sums of the inputs plus the bias term for each and every neuron in the layer, for all the instances in the batch in just one shot.\n6. Finally, if the activation parameter is set to \"relu\", the code returns *`relu(z)`* (i.e., *`max(0,z)`*), or else it just returns a linear *`z`*.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Now let\u2019s use the *`neuron_layer`* function to create the deep neural network! The first hidden layer takes *`x`* as its input. The second takes the output of the first hidden layer as its input. And finally, the output layer takes the output of the second hidden layer as its input.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 26, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.name_scope(\"dnn\"):\n    hidden1 = neuron_layer(x, num_hidden1, \"hidden1\", activation=\"relu\")\n    hidden2 = neuron_layer(hidden1, num_hidden2, \"hidden2\", activation=\"relu\")\n    logits = neuron_layer(hidden2, num_outputs, \"output\")"
        }, 
        {
            "source": "We need to define a loss function. As discussed in the last lecture, it's more numerical to estimate the cross entropy loss directly from logits using TensorFlow function:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.name_scope('evaluation'):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\\\n                                        logits=logits, name='xentropy')\n    loss = tf.reduce_mean(xentropy, name=\"loss\")\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
        }, 
        {
            "source": "*`tf.nn.sparse_softmax_cross_entropy_with_logits()`* computes the cross entropy based on the \u201clogits\u201d (i.e., the output of the network before going through the softmax activation function), and it expects labels in the form of integers ranging from 0 to the number of classes minus 1 (in our case, from 0 to 9). This will give us a 1D tensor containing the cross entropy for each instance. We can then use TensorFlow\u2019s *`reduce_mean()`* function to compute the mean cross entropy over all instances. \n\nWe also wish to estimate the accuracy of our model. For this you can use the [*`in_top_k()`* function](https://www.tensorflow.org/api_docs/python/tf/nn/in_top_k) with *k=1*. This returns a 1D tensor full of boolean values, so we need to cast these booleans to floats and then compute the average. This will give us the network\u2019s overall accuracy.\n\nNow we need to define a GradientDescentOptimizer that will tweak the model parameters to minimize the cost function.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with tf.name_scope(\"train\"):\n    learning_rate = 0.01\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    training_op = optimizer.minimize(loss)"
        }, 
        {
            "source": "Finally, we need to create a node to initialize all variables, and we will also create a *`Saver`* to\nsave our trained model parameters to disk:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "init = tf.global_variables_initializer()\nsaver = tf.train.Saver()"
        }, 
        {
            "source": "**Let's review the construction phase a little bit. We have:**\n- Created placeholders for the inputs and the targets;\n- Ceated a function to build a neuron layer and used it to create the DNN;\n- Defined the cost function and performance measure; and\n- Created an optimizer.\n\nNow move onto the execution phase.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## <span style=\"color:#0b486b\">2.2. Execution phase</span>\n\nWe first define the number of epochs that we want to run, as well as the size of the minibatches:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "num_epochs = 20\nbatch_size = 50"
        }, 
        {
            "source": "We can check our training and testing datasets", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 31, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "(55000, 784)\n(10000, 784)\n"
                }
            ], 
            "source": "print(mnist.train.images.shape)\nprint(mnist.test.images.shape)"
        }, 
        {
            "source": "Now we can train our model:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Epoch\tTrain accuracy\tTest accuracy\n0\t0.9028182029724121\t0.909500002861023\n1\t0.9251818060874939\t0.929099977016449\n2\t0.9364908933639526\t0.9379000067710876\n3\t0.94276362657547\t0.9402999877929688\n4\t0.9485999941825867\t0.944599986076355\n5\t0.9545999765396118\t0.9502999782562256\n6\t0.9583454728126526\t0.9538999795913696\n7\t0.9622727036476135\t0.9571999907493591\n8\t0.9648181796073914\t0.9589999914169312\n9\t0.9683272838592529\t0.9616000056266785\n10\t0.9705636501312256\t0.9638000130653381\n11\t0.9717090725898743\t0.9643999934196472\n12\t0.9739636182785034\t0.9653000235557556\n13\t0.9745272994041443\t0.9660000205039978\n14\t0.9761090874671936\t0.9661999940872192\n15\t0.9784545302391052\t0.9686999917030334\n16\t0.9790545701980591\t0.9697999954223633\n17\t0.9808363914489746\t0.9693999886512756\n18\t0.9820908904075623\t0.9706000089645386\n19\t0.982909083366394\t0.9721999764442444\n"
                }
            ], 
            "source": "with tf.Session() as sess:\n    init.run()\n    print(\"Epoch\\tTrain accuracy\\tTest accuracy\")\n    for epoch in range(num_epochs):\n        for iteration in range(mnist.train.num_examples // batch_size):\n            x_batch, y_batch = mnist.train.next_batch(batch_size)\n            sess.run(training_op, feed_dict={x: x_batch, y: y_batch})\n        acc_train = accuracy.eval(feed_dict={x: mnist.train.images, y: mnist.train.labels})\n        acc_test = accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels})\n        print(\"{}\\t{}\\t{}\".format(epoch, acc_train, acc_test))\n\n    save_path = saver.save(sess, \"models/example03/dnn_final.ckpt\")"
        }, 
        {
            "source": "<img src=\"https://raw.githubusercontent.com/tuliplab/mds/master/Jupyter/image/warning.png\" width=\"40\", align=\"left\"></img> If the folder containing this notebook does not contain `models` folder (which is the parent folder in the last line of code), you will get the error \n\nValueError: ``Parent directory of models/example03/dnn_final.ckpt doesn't exist, can't save.``\n\nYou **must** create `models` in the folder containing this notebook to fix the error.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## <span style=\"color:#0b486b\">2.3. Combining two phases using API functions of TensorFlow</span>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "As you might expect, TensorFlow comes with many handy functions to create standard neural network layers, so there\u2019s often no need to define your own *`neuron_layer()`* function like we just did. For example, *`tf.layers.dense()`* function creates a fully connected layer, where all the inputs are connected to all the neurons in the layer. It takes care of creating the weights and biases variables, and it set the activation argument to *`None`*, but we can set it to activation functions such as *`tf.nn.relu`*. Let\u2019s tweak the preceding code to use the *`tf.layers.dense()`* function instead of our *`neuron_layer()`* function.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "* **Building a DNN model using predefined functions in TensorFlow**:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 33, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tf.reset_default_graph()\n\nnum_inputs = 28 * 28\nnum_hidden1 = 300\nnum_hidden2 = 100\nnum_outputs = 10\nlearning_rate = 0.01\n\nx = tf.placeholder(tf.float32, shape=(None, num_inputs), name=\"x\")\ny = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n\nwith tf.name_scope(\"dnn\"):\n    hidden1 = tf.layers.dense(x, num_hidden1, name=\"hidden1\", activation=tf.nn.relu)\n    hidden2 = tf.layers.dense(hidden1, num_hidden2, name=\"hidden2\", activation=tf.nn.relu)\n    logits = tf.layers.dense(hidden2, num_outputs, name=\"outputs\")\n\nwith tf.name_scope(\"loss\"):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n    loss = tf.reduce_mean(xentropy, name=\"loss\")\n\nwith tf.name_scope(\"train\"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    training_op = optimizer.minimize(loss)\n\nwith tf.name_scope(\"eval\"):\n    correct = tf.nn.in_top_k(logits, y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n    \ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()"
        }, 
        {
            "source": "* **Training and Testing (Execution) the DNN model defined in the previous step:**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 34, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Epoch\tTrain accuracy\tTest accuracy\n0\t0.8975818157196045\t0.9049999713897705\n1\t0.9174545407295227\t0.9222000241279602\n2\t0.9289454817771912\t0.9337999820709229\n3\t0.9376727342605591\t0.9394999742507935\n4\t0.942545473575592\t0.9430000185966492\n5\t0.948199987411499\t0.9491999745368958\n6\t0.9505090713500977\t0.9501000046730042\n7\t0.9549999833106995\t0.9531999826431274\n8\t0.9589636325836182\t0.9557999968528748\n9\t0.9627272486686707\t0.9588000178337097\n10\t0.9648908972740173\t0.9603000283241272\n11\t0.9674363732337952\t0.961899995803833\n12\t0.9691272974014282\t0.9638000130653381\n13\t0.9719454646110535\t0.9642000198364258\n14\t0.9729272723197937\t0.9660000205039978\n15\t0.9741636514663696\t0.9677000045776367\n16\t0.9759636521339417\t0.9692999720573425\n17\t0.9778363704681396\t0.968999981880188\n18\t0.9785818457603455\t0.9696000218391418\n19\t0.9797636270523071\t0.9710000157356262\n"
                }
            ], 
            "source": "num_epochs = 20\nbatch_size = 50\n\nwith tf.Session() as sess:\n    init.run()\n    print(\"Epoch\\tTrain accuracy\\tTest accuracy\")\n    for epoch in range(num_epochs):\n        for iteration in range(mnist.train.num_examples // batch_size):\n            x_batch, y_batch = mnist.train.next_batch(batch_size)\n            sess.run(training_op, feed_dict={x: x_batch, y: y_batch})\n        acc_train = accuracy.eval(feed_dict={x: mnist.train.images, y: mnist.train.labels})\n        acc_test = accuracy.eval(feed_dict={x: mnist.test.images, y: mnist.test.labels})        \n        print(\"{}\\t{}\\t{}\".format(epoch, acc_train, acc_test))\n"
        }, 
        {
            "source": "### 2.4 Excercises\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "- Change five different value for the number of hidden nodes in each layer in construction step (B.1) and report the best numbers among your chosen number.\n- Increase the number of hidden layers to **three** and set five values for the number of hidden nodes then report the best value and its performance.\n- Try to change the optimizer to train the model to [Adam](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer) and [RMSProp](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer)", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark 1.6 (Unsupported)", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }, 
    "nbformat": 4
}