{
    "nbformat_minor": 2, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 03: Pattern Classification)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session C -  EM Algorithm\nIn this session, we will study EM(Expectation Maximization Algorithm) algorithm which contains expectation step and maximization step.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Motivation-for-Gaussian-Mixtures\" data-toc-modified-id=\"Motivation-for-Gaussian-Mixtures-1\">Motivation for Gaussian Mixtures</a></span></li><li><span><a href=\"#Derivation-of-Gaussian-Mixture\" data-toc-modified-id=\"Derivation-of-Gaussian-Mixture-2\">Derivation of Gaussian Mixture</a></span></li><li><span><a href=\"#Maximum-likelihood-for-Gaussian-Mixtures-&amp;-Derivation-of-the-EM-algorithm\" data-toc-modified-id=\"Maximum-likelihood-for-Gaussian-Mixtures-&amp;-Derivation-of-the-EM-algorithm-3\">Maximum likelihood for Gaussian Mixtures &amp; Derivation of the EM algorithm</a></span></li><li><span><a href=\"#How-does-the-EM-algorithm-work?\" data-toc-modified-id=\"How-does-the-EM-algorithm-work?-4\">How does the EM algorithm work?</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.-Initialization:\" data-toc-modified-id=\"1.-Initialization:-4.1\">1. Initialization:</a></span></li><li><span><a href=\"#2.-The-E-step:\" data-toc-modified-id=\"2.-The-E-step:-4.2\">2. The E step:</a></span></li><li><span><a href=\"#3.-The-M-step:\" data-toc-modified-id=\"3.-The-M-step:-4.3\">3. The M step:</a></span></li><li><span><a href=\"#4.--Evaluate-the-log-likelihood:\" data-toc-modified-id=\"4.--Evaluate-the-log-likelihood:-4.4\">4.  Evaluate the log likelihood:</a></span></li></ul></li><li><span><a href=\"#Implementation-(w/o-log-likelihood-evaluation)\" data-toc-modified-id=\"Implementation-(w/o-log-likelihood-evaluation)-5\">Implementation (w/o log likelihood evaluation)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Plotting-utils\" data-toc-modified-id=\"Plotting-utils-5.1\">Plotting utils</a></span></li><li><span><a href=\"#Example-1:-Simple-2d-Dataset\" data-toc-modified-id=\"Example-1:-Simple-2d-Dataset-5.2\">Example : Simple 2d Dataset</a></span></li>", 
            "cell_type": "markdown", 
            "metadata": {
                "toc": true
            }
        }, 
        {
            "source": "# Motivation for Gaussian Mixtures\nA single gaussian distribution is very limited for modeling data. With a gaussian mixture, a linear superposition of multiple gaussian distributions, one can often achieve a better characterization of the data.\n\n$$ p(x) = \\sum_{k=1}^{K} \\pi _{k}N(x_{n}\\mid\\mu_{k}, \\Sigma_{k}) $$\n<img src=\"Image/motivation.png\" style=\"height:400px\">\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Derivation of Gaussian Mixture\n\n-> blackboard\n\nExample:\n<img src=\"Image/gaussian_mixture.png\" style=\"height:300px\">\n\n**(a)** joint distribution $p(z)p(x|z)$ states of z corresponding to the colours => complete  \n**(b)** $p(x)$ the observed data no more knowledge about the responsible latent variables => incomplete  \n**(c)** colorur represent the responsibillities $\\gamma(z_{nk}$ for each datapoint $x_n$  ", 
            "cell_type": "markdown", 
            "metadata": {
                "ExecuteTime": {
                    "start_time": "2018-04-24T09:22:21.015881Z", 
                    "end_time": "2018-04-24T09:22:21.020401Z"
                }
            }
        }, 
        {
            "source": "# Maximum likelihood for Gaussian Mixtures & Derivation of the EM algorithm\n\n-> blackboard", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# How does the EM algorithm work? \n\nEM is an iterative process that begins with some initialization and then alternates between the expectation and maximization steps until the algorithm reaches convergence. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## 1. Initialization:\n\nInitialize the means $\\mu_k$, covariances $\\Sigma_k$ and mixing coefficients $\\pi_k$, and evaluate the initial value of the log likelihood.\n\nIt is common to run K-means first to find a suitable initialization for the EM algorithm. The covariance matricies $\\Sigma_k$ can be initialized to the sample covariances of the clusters found by K-means. The mixing coefficients $\\pi_k$ can be set to the fractions of data points assigned to the respective cluster, i.e. $$\\pi_{k_{init}} = \\frac{N_{k}}{N}$$\n\n\n<img src=\"Image/init.png\" style=\"height:250px\">", 
            "cell_type": "markdown", 
            "metadata": {
                "ExecuteTime": {
                    "start_time": "2018-04-22T15:16:50.941855Z", 
                    "end_time": "2018-04-22T15:16:50.946695Z"
                }
            }
        }, 
        {
            "source": "## 2. The E step:\n\nWe calculate the expected values $E(z_{ij})$, which is the probability that $x_i$ was drawn from the $jth$ distribution.\n    \n$$\\gamma(z_{nk}) = \\frac{\\pi _{k}N(x_{n}\\mid\\mu_{k}, \\Sigma_{k})}{\\sum_{j=1}^{k}\\pi _{j}N(x_{n}\\mid\\mu_{j}, \\Sigma_{j})}$$\n\nThe formula simply states that the expected value for $z_{ij}$ is the probability $x_i$ given $\\mu_j$ divided by the sum of the probabilities that $x_i$ belonged to each $\\mu$\n\n<img src=\"Image/expectation.png\" style=\"height:250px\">", 
            "cell_type": "markdown", 
            "metadata": {
                "ExecuteTime": {
                    "start_time": "2018-04-22T15:17:08.476230Z", 
                    "end_time": "2018-04-22T15:17:08.483470Z"
                }
            }
        }, 
        {
            "source": "## 3. The M step:\nRe-estimate the parameters using the current responsibilities\n\n$$\\mu_{k}^{new} = \\frac{1}{N_{k}}\\sum_{n=1}^{N}\\gamma(z_{nk})x_{n}$$\n$$\\Sigma_{k}^{new} = \\frac{1}{N_{k}}\\sum_{n=1}^{N}\\gamma(z_{nk})(x_{n}-\\mu_{k}^{new})(x_{n}-\\mu_{k}^{new})^{T}$$\n$$\\pi_{k}^{new} = \\frac{N_{k}}{N}$$\nwhere\n$$N_{k} = \\sum_{n=1}^{N}\\gamma(z_{kn}).$$\n\nBy repeating the E-step and M-step we are guaranteed to increase the log likelihood, so it will converge to a local Maximum. This will be discussed in the third presentation.\n\n<img src=\"Image/maximization.png\" style=\"height:250px\">", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## 4.  Evaluate the log likelihood:\n\n$$ln\\quad p(X\\mid \\mu ,\\Sigma ,\\pi ) = \\sum_{n=1}^{N}ln\\left \\{ \\sum_{k=1}^{k}\\pi_{k}N(x_{n}\\mid \\mu_{k},\\Sigma_{k}) \\right \\}$$\n\nand check for convergence of either the parameters or the log likelihood. If the convergence criterion is not satisfied return to step 2.\n\n<img src=\"Image/convergence.png\" style=\"height:250px\">", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "# Implementation (w/o log likelihood evaluation)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "ExecuteTime": {
                    "start_time": "2018-05-02T10:52:27.743533Z", 
                    "end_time": "2018-05-02T10:52:28.323615Z"
                }, 
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "import numpy as np\nfrom scipy.stats import multivariate_normal\n\nclass EM:\n    def __init__(self,X,clusters=2,init_cov_size=120):\n        self.X = X\n        self.clusters = clusters\n        datapoints = self.X.shape[0]\n        dims = self.X.shape[1]\n        self.it = 0\n        self.init_cov_size = init_cov_size\n        \n        # initialize with random points and identitiy matrices\n        # self.cluster_centers = np.random.uniform(low=self.X.min(axis=0),\n        #                                          high=self.X.max(axis=0),\n        #                                          size=(self.clusters,self.X.shape[1]))\n         \n        # init means with random points from the data; seems to result in fewer singulartities\n        rand = np.random.choice(datapoints, self.clusters,replace=False)\n        self.cluster_centers = self.X[rand,:]\n        \n        self.cluster_covs = np.stack([np.eye(dims)*self.init_cov_size]*self.clusters,axis=0)\n        self.mixing_coeffs = np.full(self.clusters,1/self.clusters)\n        \n    def fit(self,iterations=10):\n        for i in range(iterations):  \n            # Expectation\n            self.responsibilities = self._expectation(self.X)\n            # Maximization\n            self._maximization()\n            self.it += 1\n        return self.cluster_centers, self.cluster_covs\n    \n    def _expectation(self,X):\n        tripel = zip(self.cluster_centers,self.cluster_covs,self.mixing_coeffs)\n        responsibilities = np.zeros((self.clusters,X.shape[0]))\n        divisor_sum = np.zeros((X.shape[0]))\n        \n        for i,(mean,cov,mixing_coeff) in enumerate(tripel):\n            resp = mixing_coeff * multivariate_normal.pdf(X,mean,cov,allow_singular=True)\n            responsibilities[i] = resp\n            divisor_sum += resp\n        responsibilities /= divisor_sum\n        return responsibilities\n\n    \n    def _maximization(self):\n        X = self.X\n        for i,resp in enumerate(self.responsibilities):\n            Nk = resp.sum()\n            if Nk <= 1:\n                # catch near singularities\n                print(\"Singularity detected\")\n                \n                \n                # choosing new mean uniformly random\n                # new_mean = np.random.uniform(low=self.X.min(axis=0),\n                #                              high=self.X.max(axis=0))\n                \n                # choosing random points form X as mean\n                rand = np.random.choice(datapoints, self.clusters,replace=False)\n                new_mean = X[rand,:]\n                \n                new_cov = np.eye(self.X.shape[1]) * self.init_cov_size\n            else:\n                new_mean = 1/Nk * (resp[:,np.newaxis]*X).sum(axis=0)\n                unweighted_product = np.einsum('ji,jk->jik', (X-new_mean), (X-new_mean))\n                cov_sum = (resp[:,np.newaxis,np.newaxis]*unweighted_product).sum(axis=0)\n                new_cov = 1/Nk * cov_sum\n            new_mixing_coeff = Nk/X.shape[0]\n            \n            self.cluster_centers[i] = new_mean\n            self.cluster_covs[i] = new_cov\n            self.mixing_coeffs[i] = new_mixing_coeff\n                \n    def predict(self,X):\n        resp = self._expectation(X)\n        cluster_prediction = resp.argmax(axis=0)\n        prediction = np.copy(X)\n        for i,mean in enumerate(self.cluster_centers):\n            prediction[cluster_prediction==i] = mean\n        return prediction\n           "
        }, 
        {
            "source": "## Plotting utils", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "ExecuteTime": {
                    "start_time": "2018-05-02T10:52:28.325505Z", 
                    "end_time": "2018-05-02T10:52:28.587170Z"
                }, 
                "collapsed": true
            }, 
            "outputs": [], 
            "source": "from numpy import pi, sin, cos\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef oval(cov, num_points=100,radius=1):\n    arcs = np.linspace(0, 2 * pi, num_points)\n    x = radius * sin(arcs)\n    y = radius * cos(arcs)\n    \n    xy = np.array(list(zip(x, y)))\n    x, y = zip(*xy.dot(cov))\n    return x,y\n\ndef make_plot(a):\n    plt.figure(figsize=(6, 5))\n    plt.title(\"EM iteration {}\".format(a.it))\n\n    colors = ['g', 'r', 'c', 'm', 'y', 'b' ]\n    \n    # selcect elements based on expectation\n    x, y = zip(*X)\n    try:\n        plt.scatter(x, y, edgecolors=\"black\",c=a.responsibilities[0],cmap='RdYlGn')\n    except AttributeError:\n        plt.scatter(x, y, edgecolors=\"black\",color='y')\n    for i in range(a.cluster_centers.shape[0]):\n        # plot centers\n        plt.scatter(a.cluster_centers[i,0],a.cluster_centers[i,1],s=250,color=colors[i],edgecolors=\"white\")\n\n        # plot ovals that show the shape of the  variances\n        x, y = oval(a.cluster_covs[i],radius=2)\n        x += a.cluster_centers[i,0]\n        y += a.cluster_centers[i,1]\n        plt.plot(x, y,linewidth=5,color=colors[i])"
        }, 
        {
            "source": "## Example : Simple 2d Dataset", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "ExecuteTime": {
                    "start_time": "2018-05-02T10:52:28.589051Z", 
                    "end_time": "2018-05-02T10:52:28.981836Z"
                }
            }, 
            "outputs": [], 
            "source": "import pandas as pd\n\n# import dataset\nX = pd.read_csv(\"data/2d-em.csv\", header=None).as_matrix()\n# plot dataset\nx, y = zip(*X)\nplt.figure(figsize=(6, 5))\nplt.scatter(x, y, edgecolors=\"black\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": false, 
                "ExecuteTime": {
                    "start_time": "2018-05-02T10:52:28.983596Z", 
                    "end_time": "2018-05-02T10:52:31.342539Z"
                }
            }, 
            "outputs": [], 
            "source": "a = EM(X,2,init_cov_size=2)\nfor i in [1,1,1,2,5,10,10]:\n    make_plot(a)\n    mm = a.fit(i)"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "toc": {
            "nav_menu": {}, 
            "toc_window_display": false, 
            "toc_section_display": "block", 
            "skip_h1_title": false, 
            "number_sections": false, 
            "sideBar": true, 
            "toc_position": {}, 
            "toc_cell": true
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }, 
        "anaconda-cloud": {}
    }, 
    "nbformat": 4
}