{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLIP (01) Practical Data Science\n",
    "\n",
    "---\n",
    "Team Director: Shaoni Wang | snwang@tulip.academy<br />\n",
    "\n",
    "TULIP Academy <br />\n",
    "http://www.tulip.academy\n",
    "\n",
    "---\n",
    "\n",
    "## Session 31 Deriving the decision boundary\n",
    "\n",
    "\n",
    "\n",
    "[<a href=\"#sections\">back to top</a>] <br>\n",
    "\n",
    "### Bayes' Rule:\n",
    "\n",
    "\n",
    "$P(\\omega_j|x) = \\frac{p(x|\\omega_j) * P(\\omega_j)}{p(x)}$ \n",
    "\n",
    "### Discriminant Functions:\n",
    "\n",
    "The goal is to maximize the discriminant function, which we define as the posterior probability here to perform a **minimum-error classification** (Bayes classifier).\n",
    "\n",
    "$g_1(\\vec{x}) = P(\\omega_1 | \\; \\vec{x}), \\quad  g_2(\\vec{x}) = P(\\omega_2 | \\; \\vec{x})$\n",
    "\n",
    "$\\Rightarrow g_1(\\vec{x}) = P(\\vec{x}|\\;\\omega_1) \\;\\cdot\\; P(\\omega_1) \\quad | \\; ln \\\\\n",
    "\\quad g_2(\\vec{x}) = P(\\vec{x}|\\;\\omega_2) \\;\\cdot\\; P(\\omega_2) \\quad | \\; ln$\n",
    "\n",
    "<br>\n",
    "We can drop the prior probabilities (since we have equal priors in this case): \n",
    "\n",
    "$\\Rightarrow g_1(\\vec{x}) = ln(P(\\vec{x}|\\;\\omega_1))\\\\\n",
    "\\quad g_2(\\vec{x}) = ln(P(\\vec{x}|\\;\\omega_2))$\n",
    "$\\Rightarrow g_1(\\vec{x}) = \\frac{1}{2\\sigma^2} \\bigg[\\; \\vec{x}^{\\,t} - 2 \\vec{\\mu_1}^{\\,t} \\vec{x} + \\vec{\\mu_1}^{\\,t} \\bigg] \\mu_1 \\\\ \n",
    "= - \\frac{1}{2} \\bigg[ \\vec{x}^{\\,t} \\vec{x} -2 \\; [0 \\;\\; 0] \\;\\; \\vec{x} +  [0 \\;\\; 0] \\;\\; \\bigg[ \n",
    "\\begin{array}{c}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\end{array} \\bigg] \\bigg] \\\\\n",
    "= -\\frac{1}{2} \\vec{x}^{\\,t} \\vec{x}$\n",
    "\n",
    "$\\Rightarrow g_2(\\vec{x}) = \\frac{1}{2\\sigma^2} \\bigg[\\; \\vec{x}^{\\,t} - 2 \\vec{\\mu_2}^{\\,t} \\vec{x} + \\vec{\\mu_2}^{\\,t} \\bigg] \\mu_2 \\\\ \n",
    "= - \\frac{1}{2} \\bigg[ \\vec{x}^{\\,t} \\vec{x} -2 \\; 2\\;  [1 \\;\\; 1] \\;\\; \\vec{x} +  [1 \\;\\; 1] \\;\\; \\bigg[ \n",
    "\\begin{array}{c}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "\\end{array} \\bigg] \\bigg] \\\\\n",
    "= -\\frac{1}{2} \\; \\bigg[ \\; \\vec{x}^{\\,t} \\vec{x} - 2\\;  [1 \\;\\; 1] \\;\\; \\vec{x} + 2\\; \\bigg] \\;$\n",
    "\n",
    "### Decision Boundary\n",
    "\n",
    "$g_1(\\vec{x}) = g_2(\\vec{x})$ \n",
    "\n",
    "$\\Rightarrow  -\\frac{1}{2} \\vec{x}^{\\,t} \\vec{x} = -\\frac{1}{2} \\; \\bigg[ \\; \\vec{x}^{\\,t} \\vec{x} - 2\\;  [1 \\;\\; 1] \\;\\; \\vec{x} + 2\\; \\bigg] \\;$ \n",
    "\n",
    "$\\Rightarrow -2[1\\;\\; 1] \\vec{x} + 2 = 0$\n",
    "\n",
    "$\\Rightarrow [-2\\;\\; -2] \\;\\;\\vec{x} + 2 = 0$\n",
    "\n",
    "$\\Rightarrow -2x_1 - 2x_2 + 2 = 0$\n",
    "\n",
    "$\\Rightarrow -x_1 - x_2 + 1 = 0$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis\n",
    "In this part, we will do lda on a synthetic data set. That means we will generate the data ourselves and then fit a linear classifier to this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Create data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to sample 500 points each from three 2d gaussian distributions. The means of the three gaussians are $\\mu_1 = [a, b]^T$, $\\mu_2 = [a+2, b+4]^T$ and $\\mu_3 = [a+4, b]^T$ respectively, where **a** is *the last digit of your roll number* and **b** is *second last digit of your roll number*. <br>\n",
    "Similarly the covariance matrices are $\\Sigma_1 = \\Sigma_2 = \\Sigma_3 = I$ <br>\n",
    "To generate points from 2d gaussians, we should first know how to generate random numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to generate random numbers?\n",
    "use numpy random package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# code to sample a random number between 0 & 1\n",
    "# Try running this multiple times by pressing Ctrl-Enter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sympy as sp\n",
    "from sympy import *\n",
    "from numpy.linalg import solve\n",
    "\n",
    "print (np.random.random())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to sample from a gaussian?\n",
    "Use randn function to sample from a 1D gaussian with mean 0 and variance 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (np.random.randn())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's sample 1000 points!\n",
    "Use random.normal(mu, sigma, number of points). Let'us assume mean is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points = np.random.normal(3, 1, 1000)\n",
    "# A histogram plot. It looks like a gaussian distribution centered around 3\n",
    "plt.hist(points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate samples from a 2D gaussian\n",
    "Use random.multivariate_normal(mean, cov, 100) to generate 100 points from a multivariate gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = np.array([3, 3])\n",
    "cov = np.eye(2) # the identity matrix\n",
    "\n",
    "points = np.random.multivariate_normal(mean, cov, 100)\n",
    "# scatter plot with x axis as the first column of points and y axis as the second column\n",
    "plt.scatter(points[:, 0], points[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample from three different 2D gaussians\n",
    "The means of the three gaussians should be $\\mu_1 = [a, b]^T$, $\\mu_2 = [a+2, b+4]^T$ and $\\mu_3 = [a+4, b]^T$ respectively, where **a** is *the last digit of your roll number* and **b** is * the second last digit of your roll number*. <br>\n",
    "Similarly the covariance matrices are $\\Sigma_1 = \\Sigma_2 = \\Sigma_3 = I$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#covariance matrix for all 3 distributions\n",
    "cov = np.eye(2)\n",
    "\n",
    "d1 = np.random.multivariate_normal([7, 5], cov, 500)\n",
    "d2 = np.random.multivariate_normal([9, 9], cov, 500)\n",
    "d3 = np.random.multivariate_normal([11, 5], cov, 500)\n",
    "\n",
    "data = np.vstack([d1, d2, d3])\n",
    "plt.scatter(d1[:, 0], d1[:, 1], color='red')\n",
    "plt.scatter(d2[:, 0], d2[:, 1], color='blue')\n",
    "plt.scatter(d3[:, 0], d3[:, 1], color='green')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Estimate the Parameters\n",
    "##### Estimate 3 means and a covariance matrix from data\n",
    "We have assumed that $\\Sigma = \\sigma^2 I$. <br>\n",
    "Convince yourself that the Maximum Likelihood Estimate for $\\sigma^2$ is $\\frac{1}{2n}\\sum\\limits_{i=1}^n (x_i-\\mu)^T(x_i-\\mu)$, where $n$ is the number of samples. <br>\n",
    "\n",
    "Let's compute the maximum likelihood estimates for the three sets of data points (generated from 3 different gaussians) separately, denote them as $\\hat\\sigma_1^2$, $\\hat\\sigma_2^2$ and $\\hat\\sigma_3^2$ and then take the combined estimate as the averae of the three estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MLE of mean of the 3 distributions\n",
    "m1 = np.mean(d1, axis = 0)\n",
    "m2 = np.mean(d2, axis = 0)\n",
    "m3 = np.mean(d3, axis = 0)\n",
    "\n",
    "print(m1)\n",
    "print(m2)\n",
    "print(m3)\n",
    "\n",
    "#MLE of covariance of the 3 distributions\n",
    "\n",
    "t1 = d1 - m1\n",
    "s1 = np.trace(np.dot(np.transpose(t1), t1)) / (2*500)\n",
    "\n",
    "t2 = d2 - m2\n",
    "s2 = np.trace(np.dot(np.transpose(t2), t2)) / (2*500)\n",
    "\n",
    "t3 = d3 - m3\n",
    "s3 = np.trace(np.dot(np.transpose(t3), t3)) / (2*500)\n",
    "\n",
    "print(s1)\n",
    "print(s2)\n",
    "print(s3)\n",
    "\n",
    "#Combined estimate - the average of the 3 estimates\n",
    "s = (s1 + s2 + s3)/3\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Draw the Decision Boundaries\n",
    "Refer your notes/textbook to convince yourself that in the particular case where all the normal distributions have the same prior and the same covariance matrix of the form $\\sigma^2I$, the discriminant functions are given by $$g_i(x) = \\mu_i^Tx - \\frac{1}{2}\\mu_i^T\\mu_i$$Find the point at which $g_1(x) = g_2(x) = g_3(x)$ <br>\n",
    "Draw the three decision boundaries by solving $g_1(x) = g_2(x)$, $g_1(x) =  g_3(x)$ and $g_2(x) = g_3(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(d1[:, 0], d1[:, 1], color='red')\n",
    "plt.scatter(d2[:, 0], d2[:, 1], color='blue')\n",
    "plt.scatter(d3[:, 0], d3[:, 1], color='green')\n",
    "\n",
    "#Solving g1(x)=g2(x) to get the common point of intersection\n",
    "a = np.array([[m1.item(0)-m2.item(0), m1.item(1)-m2.item(1)],\n",
    "              [m1.item(0)-m3.item(0), m1.item(1)-m3.item(1)]])\n",
    "b = np.array([0.5 * ((m1.item(0) * m1.item(0)) + (m1.item(1) * m1.item(1)) - \n",
    "                     (m2.item(0) * m2.item(0)) - (m2.item(1) * m2.item(1))),\n",
    "              0.5 * ((m1.item(0) * m1.item(0)) + (m1.item(1) * m1.item(1)) - \n",
    "                     (m3.item(0) * m3.item(0)) - (m3.item(1) * m3.item(1)))])\n",
    "sol = np.linalg.solve(a, b)\n",
    "print('Common point :')\n",
    "print (sol)\n",
    "plt.scatter(sol.item(0), sol.item(1), color='black')\n",
    "\n",
    "#Plot the decision boundary g1(x)=g2(x)\n",
    "a = m1.item(0)-m2.item(0)\n",
    "b = m1.item(1)-m2.item(1)\n",
    "c = 0.5 * ((m1.item(0) * m1.item(0)) + (m1.item(1) * m1.item(1)) - \n",
    "           (m2.item(0) * m2.item(0)) - (m2.item(1) * m2.item(1)))\n",
    "x = np.linspace(2, sol.item(0), 20)\n",
    "y = (c - (a*x)) /b\n",
    "plt.plot(x, y, color='blue')   \n",
    "\n",
    "#Plot the decision boundary g1(x)=g3(x)\n",
    "a = m1.item(0)-m3.item(0)\n",
    "b = m1.item(1)-m3.item(1)\n",
    "c = 0.5 * ((m1.item(0) * m1.item(0)) + (m1.item(1) * m1.item(1)) -\n",
    "           (m3.item(0) * m3.item(0)) - (m3.item(1) * m3.item(1)))\n",
    "x = np.linspace(sol.item(0), 9.2, 10)\n",
    "y = (c - (a*x)) /b\n",
    "plt.plot(x, y, color='red')  \n",
    "\n",
    "#Plot the decision boundary g3(x)=g2(x)\n",
    "a = m3.item(0)-m2.item(0)\n",
    "b = m3.item(1)-m2.item(1)\n",
    "c = 0.5 * ((m3.item(0) * m3.item(0)) + (m3.item(1) * m3.item(1)) -\n",
    "           (m2.item(0) * m2.item(0)) - (m2.item(1) * m2.item(1)))\n",
    "x = np.linspace(sol.item(0), 15, 20)\n",
    "y = (c - (a*x)) /b\n",
    "plt.plot(x, y, color='green')   \n",
    "\n",
    "plt.axis([3,15,0,13])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
