{
    "nbformat": 4, 
    "metadata": {
        "language_info": {
            "version": "3.5.2", 
            "nbconvert_exporter": "python", 
            "name": "python", 
            "pygments_lexer": "ipython3", 
            "mimetype": "text/x-python", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "file_extension": ".py"
        }, 
        "kernelspec": {
            "language": "python", 
            "name": "python3", 
            "display_name": "Python 3.5 (Experimental) with Spark 1.6 (Unsupported)"
        }
    }, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 05: Deep Learning)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session E - Text Analytics (2) : Text Summarization and Information Extraction\n\n**The purpose of this session is to introduce how to work with textual data which are tremendously produced everyday. In this practical session, we present the following topics:**\n\n1. Revise text pre-processing techniques, also called text normalization, which involves using a variety of techniques to convert raw text into well defined sequences of linguistic components that have standard structure and notation.\n\n2. Topic modelling models.\n\n3. Automated text summarization.\n\n** References and additional reading and resources**\n- [Introduction to Topic Modeling in Python](http://chdoig.github.io/pygotham-topic-modeling/#/)\n\n---\n\n\n\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "## 0. Text Preprocessing ( or Normalization)  revisited", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "In the previous session, we have gone through basic operations for text pre-processing techniques used in text analytics application including:\n- Expanding contractions\n- Lemmatization\n- Removing special characters and symbols\n- Removing stopwords\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from contractions import CONTRACTION_MAP\nimport re # regular expression lib\n# this routine will expand the contration in texts using some pre-defined contractions and rules\n# see the list of English contractions here https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions. \n# The list of pre-defined contractions is stored in constractions.py file in CONTRACTION_MAP\n\n   \n# this function looks for each contraction and called above function\ndef expand_contractions(text, contraction_mapping):\n    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),flags=re.IGNORECASE|re.DOTALL)\n    # this function returns each expanded contraction\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n\n        expanded_contraction = contraction_mapping.get(match)\\\n                                if contraction_mapping.get(match)\\\n                                else contraction_mapping.get(match.lower())  \n\n        expanded_contraction = first_char+expanded_contraction[1:]\n        return expanded_contraction\n    \n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text    \n\nfrom nltk import pos_tag\n# nltk.download('averaged_perceptron_tagger')\nfrom nltk.corpus import wordnet as wn\nimport nltk\nimport string\n\n# Annotate text tokens with POS tags\nfrom nltk.stem import WordNetLemmatizer\nwnl = WordNetLemmatizer()\n\ndef pos_tag_text(text):\n    \n    def to_wn_tags(pos_tag):\n        if pos_tag.startswith('J'):\n            return wn.ADJ\n        elif pos_tag.startswith('V'):\n            return wn.VERB\n        elif pos_tag.startswith('N'):\n            return wn.NOUN\n        elif pos_tag.startswith('R'):\n            return wn.ADV\n        else:\n            return None\n    \n    tagged_text = pos_tag(tokenize_text(text))\n    tagged_lower_text = [(word.lower(), to_wn_tags(pos_tag))\n                         for word, pos_tag in\n                         tagged_text]\n    return tagged_lower_text\n    \n# lemmatize text based on POS tags  \ndef lemmatize_text(text):\n    \n    pos_tagged_text = pos_tag_text(text)\n    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag else word  for word, pos_tag in pos_tagged_text]\n    lemmatized_text = ' '.join(lemmatized_tokens)\n    return lemmatized_text\n\ndef tokenize_text(text):\n    tokens = nltk.word_tokenize(text) \n    tokens = [token.strip() for token in tokens]\n    return tokens\n\ndef remove_special_characters(text):\n    tokens = tokenize_text(text)\n    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n    filtered_text = ' '.join(filtered_tokens)\n    return filtered_text\n\nstopword_list = nltk.corpus.stopwords.words('english')\n\n\ndef remove_stopwords(text):\n    tokens = tokenize_text(text)\n    filtered_tokens = [token for token in tokens if token not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text\n\n# Combining all steps to normalize corpus\ndef normalize_corpus(corpus, tokenize=False):    \n    normalized_corpus = []    \n    for text in corpus: # we will process every document\n        text = expand_contractions(text, CONTRACTION_MAP)\n        text = lemmatize_text(text)\n        text = remove_special_characters(text)\n        text = remove_stopwords(text)\n        if tokenize:\n            text = tokenize_text(text)\n            normalized_corpus.append(text)\n        else:\n            normalized_corpus.append(text)\n            \n    return normalized_corpus", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "## 1. Topic modeling for information extraction", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "Topic modeling involves extracting features from document terms and using mathematical structures and frameworks like matrix factorization and SVD to generate clusters or groups of terms that are distinguishable from each other, and these cluster of\nwords form topics or concepts. These concepts can be used to interpret the main themes of a corpus and also make semantic connections among words that co-occur together frequently in various documents. In this session, we will cover the following two methods:\n- Latent semantic indexing/analysis (LSI/A)\n- Latent Dirichlet allocation (LDA)", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "We will leverage gensim and scikit-learn for our practical implementations and also look at how to build our own topic model based on latent semantic indexing. This will give you an idea of how these techniques work and also how to convert mathematical frameworks into practical implementations. We will use the following toy corpus initially to test our topic models", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "toy_corpus = [\"The fox jumps over the dog\",\n              \"The fox is very clever and quick\",\n              \"The dog is slow and lazy\",\n              \"The cat is smarter than the fox and the dog\",\n              \"Python is an excellent programming language\",\n              \"Java and Ruby are other programming languages\",\n              \"Python and Java are very popular programming languages\",\n              \"Python programs are smaller than Java programs\"]", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "You can see that we have eight documents in the preceding corpus: the first four talk about various animals, and the last four are about programming languages. Thus this shows that there are two distinct topics in the corpus. We generalized that using\nour brains, but the following sections will try to extract that same information using computational methods.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### 1.1 Latent Semantic Indexing", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "In the practical session of Week 8, we have demonstrated how to convert textual data to numeric data, e.g. BOW matrix, tf-idf feature matrix using ``sklearn``. In this session, we will now try to implement an LSI by leveraging **``gensim``** and extract topics from the toy corpus. To start, we load the necessary dependencies and normalize the toy corpus using the following code snippet", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "scrolled": false
            }, 
            "source": "from gensim import corpora, models\nimport numpy as np\n\nnorm_tokenized_corpus = normalize_corpus(toy_corpus, tokenize=True)\n\nnorm_tokenized_corpus", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We now build a dictionary or vocabulary, which gensim uses to map each unique term into a numeric value. Once built, we convert the preceding tokenized corpus into a numeric Bag of Words vector representation where each term and its frequency in a\nsentence is depicted by a tuple (term, frequency), as seen in the following snippet:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "scrolled": true
            }, 
            "source": "dictionary = corpora.Dictionary(norm_tokenized_corpus)\ndictionary.token2id", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "# convert tokenized documents into bag of words vectors\ncorpus = [dictionary.doc2bow(text) for text in norm_tokenized_corpus]\ncorpus", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# build tf-idf feature vectors\ntfidf = models.TfidfModel(corpus)\ncorpus_tfidf = tfidf[corpus]\n\n# fix the number of topics\ntotal_topics = 2\n# build the topic model\nlsi = models.LsiModel(corpus_tfidf,\n                      id2word=dictionary,\n                      num_topics=total_topics)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "Now that our topic modeling framework is built, we can see the generated topics in the following code snippet:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "scrolled": true
            }, 
            "source": "for index, topic in lsi.print_topics(total_topics):\n    print('Topic #'+str(index+1))\n    print(topic)\n    print()", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "At first, ignoring the weights, you can see that the first topic contains terms related to programming languages and\nthe second topic contains terms related to animals, which is in line with the main two concepts from our toy corpus mentioned earlier. If you now look at the weights, higher weightage and same sign exists for the terms that contribute toward each of the topics.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "Let us now look at the next technique to build topic models using latent Dirichlet allocation.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### 1.2 Latent Dirichlet Allocation", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "The latent Dirichlet allocation (LDA) technique is a generative probabilistic model where each document is assumed to have a combination of topics similar to a probabilistic latent semantic indexing model\u2014but in this case, the latent topics contain a Dirichlet prior over them. \nWe use gensim in the following implementation to build an LDA-based topic model:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "scrolled": false
            }, 
            "source": "lda = models.LdaModel(corpus_tfidf,\n                      id2word=dictionary,\n                      iterations=100,\n                      num_topics=total_topics)\n\nlda.show_topics()\n", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We see how the concepts are quite distinguishing across the two topics just as before, but note in this case the weights are positive, making it easier to interpret than LSI.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### 1.3 Word Cloud visualization for topics", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "For better representation, we can use [Wordcloud](https://github.com/amueller/word_cloud) to plot each topic and the corresponding terms and their probabilities. First we can download the correct version of word cloud at [Windows Binaries for Python Extension Packages](http://www.lfd.uci.edu/~gohlke/pythonlibs/#wordcloud), e.g. wordcloud\u20111.3.2\u2011cp36\u2011cp36m\u2011win_amd64.whl, then\n- Copying the above file to the folder containing this notebooks\n- Running the following command", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# remember to replace your file name\n!python -m pip install <filename>\n\n# !python -m pip install wordcloud-1.3.2-cp36-cp36m-win_amd64.whl\n", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We can create a word cloud for a chosen topic as follows:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "import matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\n\nfrom wordcloud import WordCloud\ntopicID=0\nweights={}\n# Extract 1000 terms from an arbitrarily chosen topic\nfor pair in lda.show_topic(topicID, topn=5):\n    weights[pair[0]]=pair[1]\n    \n# Initialize the cloud\n\nwc = WordCloud(\n    background_color=\"black\",\n    max_words=20,\n    width=256,\n    height=180,\n    relative_scaling=0,\n    stopwords=stopwords.words('english')\n)\n\n# # Generate the cloud\n\nwc.generate_from_frequencies(weights)\nwc.fit_words(weights)\nplt.imshow(wc)\nplt.axis('off')\nplt.show()\nprint(weights)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "## 2. Topic modeling with a Real-world dataset", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### 2.1 Dataset: 20 Newsgroups revisited\n\nThe 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. It was originally collected by Ken Lang, probably for his [Newsweeder: Learning to filter netnews](http://qwone.com/~jason/20Newsgroups/lang95.bib) paper, though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n\nThe data is organized into 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. *comp.sys.ibm.pc.hardware / comp.sys.mac.hardware*), while others are highly unrelated (e.g *misc.forsale / soc.religion.christian*). Here is a list of the 20 newsgroups, partitioned (more or less) according to subject matter:\n\n``comp.graphics\ncomp.os.ms-windows.misc\ncomp.sys.ibm.pc.hardware\ncomp.sys.mac.hardware\ncomp.windows.x\t\nrec.autos\nrec.motorcycles\nrec.sport.baseball\nrec.sport.hockey\t\nsci.crypt\nsci.electronics\nsci.med\nsci.space\nmisc.forsale\t\ntalk.politics.misc\ntalk.politics.guns\ntalk.politics.mideast\t\ntalk.religion.misc\nalt.atheism\nsoc.religion.christian``\n", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "### Data preparation\nWe limit to use 5 classes of documents in this section to reduce processing time. Note that we will reuse the functions defined in Section 1 and 2. First, Let us start with loading the necessary dataset and defining functions for building the training and testing datasets:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "We define two functions: ``prepare_datasets()`` to split given documents into testing and training data; ``remove_empty_docs()`` to remove empty documents:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "def prepare_datasets(corpus, labels, test_data_proportion=0.3):\n    train_X, test_X, train_Y, test_Y = train_test_split(corpus, labels, test_size=0.33,random_state=42)\n    return train_X, test_X, train_Y, test_Y\n\ndef remove_empty_docs(corpus, labels):\n    filtered_corpus = []\n    filtered_labels = []\n    for doc, label in zip(corpus, labels):\n        if doc.strip():\n            filtered_corpus.append(doc)\n            filtered_labels.append(label)\n    return filtered_corpus, filtered_labels", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We can now get the data (in case you do not have the data downloaded, feel free to connect to the Internet and take some time to download the complete corpus)", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from sklearn.datasets import fetch_20newsgroups\n\ncategories = ['alt.atheism','rec.sport.baseball','talk.politics.mideast','comp.graphics', 'sci.space']\ndataset = fetch_20newsgroups(subset='train',categories=categories, remove=('headers', 'footers', 'quotes'))\n", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "news_corpus = normalize_corpus(dataset)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from sklearn.feature_extraction.text import CountVectorizer\n\ndef bow_extractor(corpus):    \n    vectorizer = CountVectorizer()\n    features = vectorizer.fit_transform(corpus)\n    return vectorizer, features\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef tfidf_extractor(corpus, ngram_range=(1,1)):\n    vectorizer = TfidfVectorizer(min_df=1,norm='l2',smooth_idf=True,use_idf=True,ngram_range=ngram_range)\n    features = vectorizer.fit_transform(corpus)\n    return vectorizer, features", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "# bag of words features\nbow_vectorizer, bow_features = bow_extractor(news_corpus)\n", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "# tfidf features\ntfidf_vectorizer, tfidf_features = tfidf_extractor(news_corpus)", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### 2.2  Latent Semantic Indexing", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "<span style=\"color:red\">**Exercise:** </span> Applying *Latent Semantic Indexing* method to extract topics  for 20 news groups data using code given in the previous section.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "### 2.3  Latent Dirichlet Allocation", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "<span style=\"color:red\">**Exercise:** </span> Applying *Latent Dirichlet Allocation* method to extract topics for 20 news groups data using code given in the previous section.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "## 3. Automated text summarization", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "The main objective of automated document summarization is to perform this summarization without involving human inputs except for running any computer programs. Mathematical and statistical models help in building and automating the task of summarizing documents by observing their content and context. The idea of document summarization is a bit different from topic modeling. The end result is still in the form of some document, but with a few sentences based on the length we might want the summary to be. This is similar to having a research paper with an abstract or an executive summary.\n\nHere, we will be looking at summarizing text documents by utilizing document sentences, the terms in each sentence of the document, and applying SVD to them using some sort of feature weights like Bag of Words or TF-IDF weights. The core principle behind latent semantic analysis (LSA) is that in any document, there exists a latent structure among terms which are related contextually and hence should also be correlated in the same singular space.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "The input parameters we need are the number of concepts ``k`` and the number of sentences ``n`` which we want the final summary to contain:\n- Get the sentence vectors from the matrix V (k rows).\n- Get the top k singular values from S.\n- Apply a threshold-based approach to remove singular values that are less than half of the largest singular value if any exist. This is a heuristic, and you can play around with this value if you want, i.e., $S=0$ iff $S_i<\\frac{1}{2}max(S)$.\n- Multiply each term sentence column from V squared with its corresponding singular value from S also squared, to get sentence weights per topic. \n- Compute the sum of the sentence weights across the topics and take the square root of the final score to get the salience scores for each sentence in the document, i.e., $SS=\\sqrt{\\sum_{i=1}^{k}S_iV_{i}^{T}}$", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "source": "Once we have these scores, we sort them in descending order, pick the top ``n`` sentences corresponding to the highest scores,\nand combine them to form our final summary based on the order in which they were present in the original document. We will now build a generic reusable function for LSA using the previous algorithm so that we can use it on our product description document later on and you can also use this function on your own data.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom scipy.sparse.linalg import svds\n\ndef lsa_text_summarizer(documents, num_sentences=2,num_topics=2, feature_type='frequency', sv_threshold=0.5):\n    \n    vec = CountVectorizer()\n    dt_matrix=vec.fit_transform(documents).astype(float)\n                            \n    td_matrix = dt_matrix.transpose()\n    td_matrix = td_matrix.multiply(td_matrix > 0)\n\n    u, s, vt = svds(td_matrix, k=num_topics)\n\n    min_sigma_value = max(s) * sv_threshold\n    s[s < min_sigma_value] = 0\n    \n    salience_scores = np.sqrt(np.dot(np.square(s), np.square(vt)))\n\n    top_sentence_indices = salience_scores.argsort()[-num_sentences:][::-1]\n    top_sentence_indices.sort()\n    print(\"Salience scores:\\t\", np.round(salience_scores,2))\n    print(\"Selected sentences:\\t\", top_sentence_indices)\n    for index in top_sentence_indices:\n        print(sentences[index])", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We need to split a document into sentences. The function take in a text document, remove its newlines, parse the text, converting it into ASCII format, and break it down into its sentence constituents. The function is depicted in the following snippet:", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "import re\n\ndef parse_document(document):\n    document = re.sub('\\n', ' ', document)\n    if isinstance(document, str):\n        document = document\n    elif isinstance(document, unicode):\n        return unicodedata.normalize('NFKD', document).encode('ascii', 'ignore')\n    else:\n        raise ValueError('Document is not string or unicode!')\n    document = document.strip()\n    sentences = nltk.sent_tokenize(document)\n    sentences = [sentence.strip() for sentence in sentences]\n    return sentences", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "source": "We will be using our Wikipedia description of elephants as the document on which we will test all our summarization techniques.", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {
                "collapsed": true
            }, 
            "source": "import numpy as np\ntoy_text = \"\"\"\nElephants are large mammals of the family Elephantidae\nand the order Proboscidea. Two species are traditionally recognised,\nthe African elephant and the Asian elephant. Elephants are scattered\nthroughout sub-Saharan Africa, South Asia, and Southeast Asia. Male\nAfrican elephants are the largest extant terrestrial animals. All\nelephants have a long trunk used for many purposes,\nparticularly breathing, lifting water and grasping objects. Their\nincisors grow into tusks, which can serve as weapons and as tools\nfor moving objects and digging. Elephants' large ear flaps help\nto control their body temperature. Their pillar-like legs can\ncarry their great weight. African elephants have larger ears\nand concave backs while Asian elephants have smaller ears\nand convex or level backs.\n\"\"\"", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "sentences = parse_document(toy_text)\nnorm_sentences = normalize_corpus(sentences) \nvec = CountVectorizer()\ndt_matrix=vec.fit_transform(norm_sentences).astype(float)\n\n\nprint(\"Total Sentences:\", len(norm_sentences) )\n\nlsa_text_summarizer(norm_sentences, num_sentences=3,\n                    num_topics=3, feature_type='frequency',\n                    sv_threshold=0.5)  ", 
            "outputs": [], 
            "cell_type": "code"
        }
    ], 
    "nbformat_minor": 2
}