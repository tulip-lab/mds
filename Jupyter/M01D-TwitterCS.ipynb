{
    "nbformat": 4, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 01: A Touch of Data Science)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session D - Twitter Sentiment Analysis\n\n\n\nIn this article we will:\n\n* Extract twitter data using tweepy and learn how to handle it using pandas.\n* Do some basic statistics and visualizations with numpy, matplotlib and seaborn.\n* Do sentiment analysis of extracted (any user's) tweets using textblob.\n\nThe requirements that we'll need to install are:\n\n* NumPy: This is the fundamental package for scientific computing with Python. Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data.\n* Pandas: This is an open source library providing high-performance, easy-to-use data structures and data analysis tools.\n* Tweepy: This is an easy-to-use Python library for accessing the Twitter API.\n* Matplotlib: This is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.\n* Seaborn: This is a Python visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics.\n* Textblob: This is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks.\n\nAll of them are \"pip installable\". At the end of this article you'll be able to find more references about this Python libraries.\n\n\n[Adapted from GitHub](https://github.com/RodolfoFerro/pandas_twitter)\n\n\n## 1. Extracting twitter data (tweepy + pandas)\n\n### 1.1. Importing our libraries\n\nThis will be the most difficult part of all the workshop... \ud83d\ude25\nJust kidding, obviously it won't. It'll be just as easy as copying and pasting the following code in your notebook:\n\n", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# General:\nimport tweepy           # To consume Twitter's API\nimport pandas as pd     # To handle data\nimport numpy as np      # For number computing\n\n# For plotting and visualization:\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n", 
            "metadata": {}
        }, 
        {
            "source": "Excellent! We can now just run this cell of code and go to the next subsection.\n\nExcellent! We can now just run this cell of code and go to the next subsection.\n\n### 1.2. Creating a Twitter App\n\nIn order to extract tweets for a posterior analysis, we need to access to our Twitter account and create an app. The website to do this is [https://apps.twitter.com/](https://apps.twitter.com/). (If you don't know how to do this, you can follow this [tutorial video](https://www.youtube.com/watch?v=BOA7SD_09Qk) to create an account and an application.)\n\nFrom this app that we're creating we will save the following information in a script called `credentials.py`:\n* Consumer Key (API Key)\n* Consumer Secret (API Secret)\n* Access Token\n* Access Token Secret\n\nAn example of this script is the following:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Twitter App access keys for @user\n\n# Consume:\nCONSUMER_KEY    = ''\nCONSUMER_SECRET = ''\n\n# Access:\nACCESS_TOKEN  = ''\nACCESS_SECRET = ''\n", 
            "metadata": {}
        }, 
        {
            "source": "The reason of creating this extra file is that we want to export only the value of this variables, but being unseen in our main code (our notebook). We are now able to consume Twitter's API. In order to do this, we will create a function to allow us our keys authentication. We will add this function in another cell of code and we will run it:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "#!pip install credentials", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# We import our access keys:\nfrom credentials import *    # This will allow us to use the keys as variables\n\n# API's setup:\ndef twitter_setup():\n    \"\"\"\n    Utility function to setup the Twitter's API\n    with our access keys provided.\n    \"\"\"\n    # Authentication and access using keys:\n    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n\n    # Return API with authentication:\n    api = tweepy.API(auth)\n    return api\n", 
            "metadata": {}
        }, 
        {
            "source": "\nSo far, so easy right? We're good to extract tweets in the next section.\n\n\n### 1.3. Tweets extraction\n\nNow that we've created a function to setup the Twitter API, we can use this function to create an \"*extractor*\" object. After this, we will use Tweepy's function `extractor.user_timeline(screen_name, count)` to extract from `screen_name`'s user the quantity of `count` tweets.\n\nWe can choose any user such as [@realDonaldTrump](https://twitter.com/realDonaldTrump) to extract data for a posterior analysis. Yeah, we wanna keep it interesting, LOL.\n\nThe way to extract Twitter's data is as follows:\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# We create an extractor object:\nextractor = twitter_setup()\n\n# We create a tweet list as follows:\ntweets = extractor.user_timeline(screen_name=\"realDonaldTrump\", count=200)\nprint(\"Number of tweets extracted: {}.\\n\".format(len(tweets)))\n\n# We print the most recent 5 tweets:\nprint(\"5 recent tweets:\\n\")\nfor tweet in tweets[:5]:\n    print(tweet.text)\n    print()\n", 
            "metadata": {}
        }, 
        {
            "source": "With this we will have an output similar to this one, and we are able to compare the output with the Twitter account.\n\nWe now have an extractor and extracted data, which is listed in the `tweets` variable. I must mention at this point that each element in that list is a `tweet` object from Tweepy, and we will learn how to handle this data in the next subsection.\n\n### 1.4. Creating a (pandas) DataFrame\n\nWe now have initial information to construct a pandas `DataFrame`, in order to manipulate the info in a very easy way.\n\nIPython's `display` function plots an output in a friendly way, and the `head`method of a dataframe allows us to visualize the first 5 elements of the dataframe (or the first number of elements that are passed as an argument).\n\nSo, using Python's list comprehension:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# We create a pandas dataframe as follows:\ndata = pd.DataFrame(data=[tweet.text for tweet in tweets], columns=['Tweets'])\n\n# We display the first 10 elements of the dataframe:\ndisplay(data.head(10))\n", 
            "metadata": {}
        }, 
        {
            "source": "So we now have a nice table with ordered data.\n\nAn interesting thing is the number if internal methods that the `tweet`structure has in Tweepy:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Internal methods of a single tweet object:\nprint(dir(tweets[0]))\n", 
            "metadata": {}
        }, 
        {
            "source": "The interesting part from here is the quantity of metadata contained in a single tweet. If we want to obtain data such as the creation date, or the source of creation, we can access the info with this attributes. An example is the following:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# We print info from the first tweet:\nprint(tweets[0].id)\nprint(tweets[0].created_at)\nprint(tweets[0].source)\nprint(tweets[0].favorite_count)\nprint(tweets[0].retweet_count)\nprint(tweets[0].geo)\nprint(tweets[0].coordinates)\nprint(tweets[0].entities)\n", 
            "metadata": {}
        }, 
        {
            "source": "We're now able to order the relevant data and add it to our dataframe.\n\n\n### 1.5. Adding relevant info to our dataframe\n\nAs we can see, we can obtain a lot of data from a single tweet. But not all this data is always useful for specific stuff. In our case we well just add some data to our dataframe. For this we will use Pythons list comprehension and a new column will be added to the dataframe by just simply adding the name of the content between square brackets and assign the content. The code goes as...:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# We add relevant data:\ndata['len']  = np.array([len(tweet.text) for tweet in tweets])\ndata['ID']   = np.array([tweet.id for tweet in tweets])\ndata['Date'] = np.array([tweet.created_at for tweet in tweets])\ndata['Source'] = np.array([tweet.source for tweet in tweets])\ndata['Likes']  = np.array([tweet.favorite_count for tweet in tweets])\ndata['RTs']    = np.array([tweet.retweet_count for tweet in tweets])\n", 
            "metadata": {}
        }, 
        {
            "source": "And to display again the dataframe to see the changes we just...:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Display of first 10 elements from dataframe:\ndisplay(data.head(10))\n", 
            "metadata": {}
        }, 
        {
            "source": "Now that we have extracted and have the data in a easy-to-handle ordered way, we're ready to do a bit more of manipulation to visualize some plots and gather some statistical data. The first part of the workshop is done.\n\n## 2. Visualization and basic statistics\n\n### 2.1. Averages and popularity\n\nWe first want to calculate some basic statistical data, such as the mean of the length of characters of all tweets, the tweet with more likes and retweets, etc.\n\nFrom now, I'll just add some input code and the output right below the code.\n\nTo obtain the mean, using numpy:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# We extract the mean of lenghts:\nmean = np.mean(data['len'])\n\nprint(\"The lenght's average in tweets: {}\".format(mean))\n", 
            "metadata": {}
        }, 
        {
            "source": "To extract more data, we will use some pandas' functionalities:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# We extract the tweet with more FAVs and more RTs:\n\nfav_max = np.max(data['Likes'])\nrt_max  = np.max(data['RTs'])\n\nfav = data[data.Likes == fav_max].index[0]\nrt  = data[data.RTs == rt_max].index[0]\n\n# Max FAVs:\nprint(\"The tweet with more likes is: \\n{}\".format(data['Tweets'][fav]))\nprint(\"Number of likes: {}\".format(fav_max))\nprint(\"{} characters.\\n\".format(data['len'][fav]))\n\n# Max RTs:\nprint(\"The tweet with more retweets is: \\n{}\".format(data['Tweets'][rt]))\nprint(\"Number of retweets: {}\".format(rt_max))\nprint(\"{} characters.\\n\".format(data['len'][rt]))\n", 
            "metadata": {}
        }, 
        {
            "source": "This is common, but it won't necessarily happen: the tweet with more likes is the tweet with more retweets. What we're doing is that we find the maximum number of likes from the *'Likes'* column and the maximum number of retweets from the *'RTs'* using numpy's `max` function. With this we just look for the index in each of both columns that satisfy to be the maximum. Since more than one could have the same number of likes/retweets (the maximum) we just need to take the first one found, and that's why we use `.index[0]` to assign the index to the variables `fav`and `rt`. To print the tweet that satisfies, we access the data in the same way we would access a matrix or any indexed object.\n\nWe're now ready to plot some stuff. :)\n\n### 2.2. Time series\n\nPandas has its own object for time series. Since we have a whole vector with creation dates, we can construct time series respect tweets lengths, likes and retweets.\n\nThe way we do it is:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# We create time series for data:\n\ntlen = pd.Series(data=data['len'].values, index=data['Date'])\ntfav = pd.Series(data=data['Likes'].values, index=data['Date'])\ntret = pd.Series(data=data['RTs'].values, index=data['Date'])\n", 
            "metadata": {}
        }, 
        {
            "source": "And if we want to plot the time series, pandas already has its own method in the object. We can plot a time series as follows:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Lenghts along time:\ntlen.plot(figsize=(16,4), color='r');\n", 
            "metadata": {}
        }, 
        {
            "source": "And to plot the likes versus the retweets in the same chart:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# Likes vs retweets visualization:\ntfav.plot(figsize=(16,4), label=\"Likes\", legend=True)\ntret.plot(figsize=(16,4), label=\"Retweets\", legend=True);\n", 
            "metadata": {}
        }, 
        {
            "source": "### 2.3. Pie charts of sources\n\nWe're almost done with this second section of the workshop. Now we will plot the sources in a pie chart, since we realized that not every tweet is tweeted from the same source (\ud83d\ude31\ud83e\udd14). We first clean all the sources:\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# We obtain all possible sources:\nsources = []\nfor source in data['Source']:\n    if source not in sources:\n        sources.append(source)\n\n# We print sources list:\nprint(\"Creation of content sources:\")\nfor source in sources:\n    print(\"* {}\".format(source))\n", 
            "metadata": {}
        }, 
        {
            "source": "With the following output, we realize that basically this twitter account has two sources:\n<code>Creation of content sources:</code>\n<code>* Twitter for iPhone</code>\n<code>* Media Studio</code>\n\nWe now count the number of each source and create a pie chart. You'll notice that this code cell is not the most optimized one... Please have in mind that it was 4 in the morning when I was designing this workshop. \ud83d\ude05\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# We create a numpy vector mapped to labels:\npercent = np.zeros(len(sources))\n\nfor source in data['Source']:\n    for index in range(len(sources)):\n        if source == sources[index]:\n            percent[index] += 1\n            pass\n\npercent /= 100\n\n# Pie chart:\npie_chart = pd.Series(percent, index=sources, name='Sources')\npie_chart.plot.pie(fontsize=11, autopct='%.2f', figsize=(6, 6));\n", 
            "metadata": {}
        }, 
        {
            "source": "## 3. Sentiment analysis\n\n### 3.1. Importing textblob\n\nAs we mentioned at the beginning of this workshop, textblob will allow us to do sentiment analysis in a very simple way. We will also use the `re` library from Python, which is used to work with *regular expressions*. For this, I'll provide you two utility functions to: a) clean text (which means that any symbol distinct to an alphanumeric value will be remapped into a new one that satisfies this condition), and b) create a classifier to analyze the polarity of each tweet after cleaning the text in it. I won't explain the specific way in which the function that cleans works, since it would be extended and it might be better understood in the [official `re`documentation](https://docs.python.org/3/library/re.html).\n\nThe code that I'm providing is:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "from textblob import TextBlob\nimport re\n\ndef clean_tweet(tweet):\n    '''\n    Utility function to clean the text in a tweet by removing\n    links and special characters using regex.\n    '''\n    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n\ndef analize_sentiment(tweet):\n    '''\n    Utility function to classify the polarity of a tweet\n    using textblob.\n    '''\n    analysis = TextBlob(clean_tweet(tweet))\n    if analysis.sentiment.polarity > 0:\n        return 1\n    elif analysis.sentiment.polarity == 0:\n        return 0\n    else:\n        return -1\n", 
            "metadata": {}
        }, 
        {
            "source": "The way it works is that textblob already provides a trained analyzer (cool, right?). Textblob can work with different *machine learning* models used in *natural language processing*. If you want to train your own classifier (or at least check how it works) feel free to check the following [link](https://textblob.readthedocs.io/en/dev/classifiers.html). It might result relevant since we're working with a pre-trained model (for which we don't not the data that was used).\n\nAnyway, getting back to the code we will just add an extra column to our data. This column will contain the sentiment analysis and we can plot the dataframe to see the update:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# We create a column with the result of the analysis:\ndata['SA'] = np.array([ analize_sentiment(tweet) for tweet in data['Tweets'] ])\n\n# We display the updated dataframe with the new column:\ndisplay(data.head(10))\n", 
            "metadata": {}
        }, 
        {
            "source": "As we can see, the last column contains the sentiment analysis (`SA`). We now just need to check the results.\n\n### 3.2. Analyzing the results\n\nTo have a simple way to verify the results, we will count the number of neutral, positive and negative tweets and extract the percentages.\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# We construct lists with classified tweets:\n\npos_tweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] > 0]\nneu_tweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] == 0]\nneg_tweets = [ tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] < 0]\n", 
            "metadata": {}
        }, 
        {
            "source": "Now that we have the lists, we just print the percentages:\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "outputs": [], 
            "execution_count": null, 
            "cell_type": "code", 
            "source": "# We print percentages:\n\nprint(\"Percentage of positive tweets: {}%\".format(len(pos_tweets)*100/len(data['Tweets'])))\nprint(\"Percentage of neutral tweets: {}%\".format(len(neu_tweets)*100/len(data['Tweets'])))\nprint(\"Percentage de negative tweets: {}%\".format(len(neg_tweets)*100/len(data['Tweets'])))\n", 
            "metadata": {}
        }, 
        {
            "source": "We have to consider that we're working only with the 200 most recent tweets from D. Trump (last updated: September 2nd.). For more accurate results we can consider more tweets. An interesting thing (an invitation to the readers) is to analyze the polarity of the tweets from different sources, it might be deterministic that by only considering the tweets from one source the polarity would result more positive/negative. Anyway, I hope this resulted interesting.\n\nAs we saw, we can extract, manipulate, visualize and analyze data in a very simple way with Python. I hope that this leaves some uncertainty in the reader, for further exploration using this tools.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }
    ], 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython2", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "mimetype": "text/x-python", 
            "name": "python", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }, 
        "kernelspec": {
            "language": "python", 
            "display_name": "Python 2 with Spark 2.1", 
            "name": "python2-spark21"
        }
    }, 
    "nbformat_minor": 1
}