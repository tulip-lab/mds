{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "# Modern Data Science \n**(Module 05: Deep Learning)**\n\n---\n- Materials in this module include resources collected from various open-source online repositories.\n- You are free to use, change and distribute this package.\n\nPrepared by and for \n**Student Members** |\n2006-2018 [TULIP Lab](http://www.tulip.org.au), Australia\n\n---\n\n\n# Session K - Restricted Boltzmann Machines\n\n<font size = 3><strong>In this notebook we will overview the basics of RMB</strong></font>\n<br>\n- <p><a href=\"#ref1\">Initialization</a></p>\n- <p><a href=\"#ref2\">RBM Layers</a></p>\n- <p><a href=\"#ref3\">How an RBM works in Deep Learning</a></p>\n- <p><a href=\"#ref4\">RBM training</a></p>\n<p></p>\n\n\n** References and additional reading and resources**\n- https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine  \n- http://deeplearning.net/tutorial/rbm.html  \n- http://deeplearning4j.org/restrictedboltzmannmachine.html  \n- http://imonad.com/rbm/restricted-boltzmann-machine/  \n\n* This material is from [CognitiveClass.AI](https://cognitiveclass.ai)\n* Created by: \n<a href = \"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a>, Gabriel Garcez Barros Souza\n* This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n\n\n\n---\n\n\n## 1. Introduction\n\n__Restricted Boltzmann Machine (RBM):__  RBMs are shallow neural nets that learn to reconstruct data by themselves in an unsupervised fashion.  \n\n\n\n#### 1.1 How does it work?\n\nSimply, RBM takes the inputs and translates them to a set of numbers that represents them. Then, these numbers can be translated back to reconstruct the inputs. Through several forward and backward passes, the RBM will be trained, and a trained RBM can reveal which features are the most important ones when detecting patterns.   \n\n#### Why are RBMs important?\nIt can automatically extract __meaningful__ features from a given input.\n\n#### What are the applications of RBM?\nRBM is useful for <a href='http://www.cs.utoronto.ca/~hinton/absps/netflixICML.pdf'>  Collaborative Filtering</a>, dimensionality reduction, classification, regression, feature learning, topic modeling and even __Deep Belief Networks__.\n\n\n\n#### Is RBM a generative model?\n\nRBM is a generative model. What is a generative model?\n\nFirst, lets see what is different betwee discriminative and generative model: \n\n__Discriminative:__Consider a classification problem in which we want to learn to distinguish between Sedan cars (y = 1) and SUV cars (y = 0), based on some features of an cars. Given a training set, an algorithm like logistic regression tries to find a straight line\u2014that is, a decision boundary\u2014that separates the suv and sedan. \n__Generative:__ looking at cars, we can build a model of what Sedan cars look like. Then, looking at SUVs, we can build a separate model of what SUV cars look like. Finally, to classify a new car, we\ncan match the new car against the Sedan model, and match it against the SUV model, to see whether the new car looks more like the SUV or Sedan. \n\nGenerative Models specify a probability distribution over a dataset of input vectors. we can do both supervise and unsupervise tasks with generative models:\n- In an unsupervised task, we try to form a model for P(x), where x is an input vector. \n- In the supervised task, we first form a model for P(x|y), where y is the label for x. For example, if y indicates whether an example is a SUV (0) or a Sedan (1), then p(x|y = 0) models the distribution of SUVs\u2019 features, and p(x|y = 1) models the distribution of Sedans\u2019 features. If we manage to find P(x|y) and P(y), then we can use `bayes rule` to estimate P(y|x), because: p(y|x) = p(x|y)p(y)/p(x)\n\nCan we build a generative model, and then use it to create synthetic data by directly sampling from the modelled probability distributions? Lets see. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"ref1\"></a>\n## 2. Initialization\n\nFirst we have to load the utility file which  contains different utility functions that are not connected\nin any way to the networks presented in the tutorials, but rather help in\nprocessing the outputs into a more understandable way.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import urllib\nresponse = urllib.urlopen('http://deeplearning.net/tutorial/code/utils.py')\ncontent = response.read()\ntarget = open('utils.py', 'w')\ntarget.write(content)\ntarget.close()"
        }, 
        {
            "source": "Now, we load in all the packages that we use to create the net including the TensorFlow package:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import tensorflow as tf\nimport numpy as np\nfrom tensorflow.examples.tutorials.mnist import input_data\n#!pip install pillow\nfrom PIL import Image\n#import Image\nfrom utils import tile_raster_images\nimport matplotlib.pyplot as plt\n%matplotlib inline"
        }, 
        {
            "source": "We will be using the MINST dataset to practice the usage of RBMs. The following cell loads the MINST dataset.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\ntrX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"
        }, 
        {
            "source": "----------------", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"ref2\"></a>\n## 3. RBM layers\n\nAn RBM has two layers. The first layer of the RBM is called the __visible__ (or input layer). MNIST images have 784 pixels, so the visible layer must have 784 input nodes. \n\nThe second layer is the __hidden__ layer, which possesses i neurons in our case. Each hidden unit has a binary state, which we\u2019ll call it __si__, and turns either on or off (i.e., si = 1 or si = 0) with a probability that is a logistic function of the inputs it receives from the other j visible units, called for example, p(si = 1). For our case, we'll use 500 nodes in the hidden layer, so i = 500.\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "     \n\nEach node in the first layer also has a __bias__. We will denote the bias as \u201cvb\u201d for the visible units. The _vb_ is shared among all visible units.\n\nHere we define the  __bias__ of second layer as well. We will denote the bias as \u201chb\u201d for the hidden units. The _hb_ is shared among all visible units", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "vb = tf.placeholder(\"float\", [784])\nhb = tf.placeholder(\"float\", [500])"
        }, 
        {
            "source": "We have to define weights among the input layer and hidden layer nodes. In the weight matrix, the rows are equal to the input nodes, and the columns are equal to the output nodes. Let __W__ be the Tensor of 784x500 (784 - number of visible neurons, 500 - number of hidden neurons) that represents weights between neurons. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "W = tf.placeholder(\"float\", [784, 500])"
        }, 
        {
            "source": "----------------", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"ref3\"></a>\n## 4. What RBM can do after training?\n\nThink RBM as a model that have been trained, and now it can calculate the probability of observing a case (e.g. wet road) given some hidden/latent values (e.g. raining). That is, the RBM can be viewed as a generative model that assigns a probability to each possible `binary state vectors` over its visible units (v). \n\nWhat are the possible __binary states vectors__? \n- The visible layer can have different binary states, or so called, configurations. For example, in the 7 unit visible layer (above photo), it has ${2^{7}}$ differnt configurations, and each configuration has its probablity (assuming we dont have any bias).\n    - (0,0,0,0,0,0,0) --> p(config1)=p(v1)=p(s1=0,s2=0, .., s7=0)\n    - (0,0,0,0,0,0,1) --> p(config2)=p(v2)=p(s1=0,s2=1, .., s7=1)\n    - (0,0,0,0,0,1,0) --> p(config3)=p(v3)=p(s1=1,s2=0, .., s7=0)\n    - (0,0,0,0,0,1,1) --> p(config4)=p(v4)=p(s1=1,s2=1, .., s7=1)\n    - etc.\n\n\nSo, for example if we have 784 units in the visible layer, it will generates a probability distribution over all the ${2^{784}}$ possible visible vectors, i.e, p(v). \n\nNow, it would be really cool, if a model (after training) can calculate the probablity of visible layer, given hidden layer values.\n\n## 5. How to train an RBM? \n\nRBM has two phases: 1) Forward Pass, and  2) Backward Pass or Reconstruction:\n\n\n__Phase 1) Forward pass:__  Processing happens in each node in the hidden layer. That is, input data from all visible nodes are being passed to all hidden nodes. This computation begins by making stochastic decisions about whether to transmit that input or not (i.e. to determine the state of each hidden layer). At the hidden layer's nodes, __X__ is multiplied by a __W__ and added to __h_bias__. The result of those two operations is fed into the sigmoid function, which produces the node\u2019s output/state. As a result, one output is produced for each hidden node. So, for each row in the training set, __a tensor of probabilities__ is generated, which in our case it is of size [1x500], and totally 55000 vectors (_h0_=[55000x500]).\n\n\nThen, we take the tensor of probabilities (as from a sigmoidal activation) and make samples from all the distributions, __h0__.  That is, we sample the activation vector from the probability distribution of hidden layer values. Samples are used to estimate the negative phase gradient which will be explained later.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "X = tf.placeholder(\"float\", [None, 784])\n_h0= tf.nn.sigmoid(tf.matmul(X, W) + hb)  #probabilities of the hidden units\nh0 = tf.nn.relu(tf.sign(_h0 - tf.random_uniform(tf.shape(_h0)))) #sample_h_given_X"
        }, 
        {
            "source": "Before we go further, let's look at an example of sampling:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "with  tf.Session() as sess:\n    a= tf.constant([0.7, 0.1, 0.8, 0.2])\n    print sess.run(a)\n    b=sess.run(tf.random_uniform(tf.shape(a)))\n    print b\n    print sess.run(a-b)\n    print sess.run(tf.sign( a - b))\n    print sess.run(tf.nn.relu(tf.sign( a - b)))"
        }, 
        {
            "source": "__Phase 2) Backward Pass (Reconstruction):__\nThe RBM reconstructs data by making several forward and backward passes between the visible and hidden layers.\n\nSo, in the second phase (i.e. reconstruction phase), the samples from the hidden layer (i.e. h0) play the role of input. That is, __h0__ becomes the input in the backward pass. The same weight matrix and visible layer biases are used to go through the sigmoid function. The produced output is a reconstruction which is an approximation of the original input.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "_v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb) \nv1 = tf.nn.relu(tf.sign(_v1 - tf.random_uniform(tf.shape(_v1)))) #sample_v_given_h\nh1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb)"
        }, 
        {
            "source": "Reconstruction steps: \n- Get one data point from data set, like _x_, and pass it through the net\n- Pass 0: (x)  -> (x:-:_h0) -> (h0:-:v1)   (v1 is reconstruction of the first pass)\n- Pass 1: (v1) -> (v1:-:h1) -> (_h0:-:v2)   (v2 is reconstruction of the second pass)\n- Pass 2: (v2) -> (v2:-:h2) -> (_h1:-:v3)   (v3 is reconstruction of the third pass)\n- Pass n: (vn) -> (vn:-:hn+1) -> (_hn:-:vn+1)(vn is reconstruction of the nth pass)", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "----------------", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "<a id=\"ref4\"></a>\n### How to calculate gradients?\nIn order to train an RBM, we have to maximize the product of probabilities assigned to the training set V (a matrix, where each row of it is treated as a visible vector v):\n<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/d42e9f5aad5e1a62b11b119c9315236383c1864a\" >\n\n\nOr equivalently, maximize the expected log probability of V:\n<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/ba0ceed99dca5ff1d21e5ace23f5f2223f19efc0\" >\n\nEquivalently, we can define the objective function as __the average negative log-likelihood__ and try to minimize it. To achieve this, we need the partial derivative of this function in respect to all of its parameters. And it can be shown that the above equation is indirectly the weights and biases function, so minimization of the objective function here means modifying/optimizing the weight vector W. So, we can use __stochastic gradient descent__ to find the optimal weight and consequently minimize the objective function. When we derive, it give us 2 terms, called positive and negative gradient. These negative and positive phases reflect their effect on the probability density defined by the model. The positive one depends on observations (X), and the second one depends on only the model. \n \nThe __Positive phase__ increases the probability of training data.  \nThe __Negative phase__ decreases the probability of samples generated by the model.  \n\nThe negative phase is hard to compute, so we use a method called __Contrastive Divergence (CD)__ to approximate it.  It is designed in such a way that at least the direction of the gradient estimate is somewhat accurate, even when the size is not (In real world models, more accurate techniques like CD-k or PCD are used to train RBMs). During the calculation of CD, we have to use __Gibbs sampling__ to sample from our model distribution.    \n\nContrastive Divergence is actually matrix of values that is computed and used to adjust values of the W matrix. Changing W incrementally leads to training of W values. Then on each step (epoch), W is updated to a new value W' through the equation below:\n$W' = W + alpha * CD$ \n\n__ What is Alpha?__  \nHere, alpha is some small step rate and is also known as the \"learning rate\".\n\n__How can we calculate CD?__  \nWe can perform single-step Contrastive Divergence (CD-1) taking the following steps:\n\n1. Take a training sample from X, compute the probabilities of the hidden units and sample a hidden activation vector h0 from this probability distribution.\n - $\\_h0 = sigmoid(X \\otimes W + hb)$\n - $h0 = sampleProb(h0)$\n2. Compute the [outer product](https://en.wikipedia.org/wiki/Outer_product) of X and h0 and call this the positive gradient.\n - $w\\_pos\\_grad = X \\otimes h0$  (Reconstruction in the first pass)  \n3. From h, reconstruct v1, and then take a sample of the visible units, then resample the hidden activations h1 from this. (**Gibbs sampling step**)\n - $\\_v1 = sigmoid(h0 \\otimes transpose(W) + vb)$\n - $v1 = sample_prob(v1)$  (Sample v given h)\n - $h1 = sigmoid(v1 \\otimes W + hb)$\n4. Compute the outer product of v1 and h1 and call this the negative gradient.\n - $w\\_neg\\_grad = v1 \\otimes h1$  (Reconstruction 1)\n5. Now, CD equals the positive gradient minus the - negative gradient, CD is a matrix of size 784x500. \n - $CD = (w\\_pos\\_grad - w\\_neg\\_grad) / datapoints$\n6. Update the weight to be CD times some learning rate\n - $W' = W + alpha*CD$\n7. At the end of the algorithm, the visible nodes will store the value of the sample.\n\n#### What is sampling here (sampleProb)?\nIn forward pass: We randomly set the values of each hi to be 1 with probability $sigmoid(v \\otimes W + hb)$.  \nIn reconstruction: We randomly set the values of each vi to be 1 with probability $ sigmoid(h \\otimes transpose(W) + vb)$.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "alpha = 1.0\nw_pos_grad = tf.matmul(tf.transpose(X), h0)\nw_neg_grad = tf.matmul(tf.transpose(v1), h1)\nCD = (w_pos_grad - w_neg_grad) / tf.to_float(tf.shape(X)[0])\nupdate_w = W + alpha * CD\nupdate_vb = vb + alpha * tf.reduce_mean(X - v1, 0)\nupdate_hb = hb + alpha * tf.reduce_mean(h0 - h1, 0)"
        }, 
        {
            "source": "### What is objective function?\n\n__Goal: Maximize the likelihood of our data being drawn from that distribution__\n\n__Calculate error:__  \nIn each epoch, we compute the \"error\" as a sum of the squared difference between step 1 and step n,\ne.g the error shows the difference between the data and its reconstruction.\n\n__Note:__ tf.reduce_mean computes the mean of elements across dimensions of a tensor.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "err = tf.reduce_mean(tf.square(X - v1))"
        }, 
        {
            "source": "Let's start a session and initialize the variables:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "cur_w = np.zeros([784, 500], np.float32)\ncur_vb = np.zeros([784], np.float32)\ncur_hb = np.zeros([500], np.float32)\nprv_w = np.zeros([784, 500], np.float32)\nprv_vb = np.zeros([784], np.float32)\nprv_hb = np.zeros([500], np.float32)\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)"
        }, 
        {
            "source": "Let look at the error of the first run:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sess.run(err, feed_dict={X: trX, W: prv_w, vb: prv_vb, hb: prv_hb})"
        }, 
        {
            "source": "To recall, the whole algorithm works as:  \n- For each epoch, and for each batch do:\n  - Compute CD as: \n     - For each data point in batch do:\n        - w_pos_grad = 0, w_neg_grad= 0 (matrices)\n        - Pass data point through net, calculating v (reconstruction) and h\n        - update w_pos_grad = w_pos_grad + X$\\otimes$h0\n        - update w_neg_grad = w_neg_grad + v1$\\otimes$h1\n     - CD = average of pos_grad and neg_grad by dividing them by the amount of data points.\n  - Update weights and biases W' = W + alpha * CD \n  - Calculate error\n- Repeat for the next epoch until error is small or after some fixed number of epochs.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "#Parameters\nepochs = 5\nbatchsize = 100\nweights = []\nerrors = []\n\nfor epoch in range(epochs):\n    for start, end in zip( range(0, len(trX), batchsize), range(batchsize, len(trX), batchsize)):\n        batch = trX[start:end]\n        cur_w = sess.run(update_w, feed_dict={ X: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n        cur_vb = sess.run(update_vb, feed_dict={  X: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n        cur_hb = sess.run(update_hb, feed_dict={ X: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n        prv_w = cur_w\n        prv_vb = cur_vb\n        prv_hb = cur_hb\n        if start % 10000 == 0:\n            errors.append(sess.run(err, feed_dict={X: trX, W: cur_w, vb: cur_vb, hb: cur_hb}))\n            weights.append(cur_w)\n    print 'Epoch: %d' % epoch,'reconstruction error: %f' % errors[-1]\nplt.plot(errors)\nplt.xlabel(\"Batch Number\")\nplt.ylabel(\"Error\")\nplt.show()"
        }, 
        {
            "source": "What is the last weight after training?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "uw = weights[-1].T\nprint uw # a weight matrix of shape (500,784)"
        }, 
        {
            "source": "We can take each hidden unit and visualize the connections between that hidden unit and each element in the input vector. ", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "Let's plot the current weights:\n__tile_raster_images__ helps in generating a easy to grasp image from a set of samples or weights. It transform the __uw__ (with one flattened image per row of size 784), into an array (of size $25*20$) in which images are reshaped and layed out like tiles on a floor.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tile_raster_images(X=cur_w.T, img_shape=(28, 28), tile_shape=(25, 20), tile_spacing=(1, 1))\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n%matplotlib inline\nimage = Image.fromarray(tile_raster_images(X=cur_w.T, img_shape=(28, 28) ,tile_shape=(25, 20), tile_spacing=(1, 1)))\n### Plot image\nplt.rcParams['figure.figsize'] = (18.0, 18.0)\nimgplot = plt.imshow(image)\nimgplot.set_cmap('gray')  "
        }, 
        {
            "source": "Each tile in the above visualization corresponds to a vector of connections between a hidden unit and visible layer's units. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Let's look at one of the learned weights corresponding to one of hidden units for example. In this particular square, the gray color represents weight = 0, and the whiter it is, the more positive the weights are (closer to 1). Conversely, the darker pixels are, the more negative the weights. The positive pixels will increase the probability of activation in hidden units (after muliplying by input/visible pixels), and negative pixels will decrease the probability of a unit hidden to be 1 (activated). So, why is this important?  So we can see that this specific square (hidden unit) can detect a feature (e.g. a \"/\" shape) and if it exists in the input.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from PIL import Image\nimage = Image.fromarray(tile_raster_images(X=cur_w.T[10:11], img_shape=(28, 28),tile_shape=(1, 1), tile_spacing=(1, 1)))\n### Plot image\nplt.rcParams['figure.figsize'] = (4.0, 4.0)\nimgplot = plt.imshow(image)\nimgplot.set_cmap('gray')  "
        }, 
        {
            "source": "Let's look at the reconstruction of an image.\n\nFirst we plot one of images:", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sample_case = trX[1:2]\nimg = Image.fromarray(tile_raster_images(X=sample_case, img_shape=(28, 28),tile_shape=(1, 1), tile_spacing=(1, 1)))\nplt.rcParams['figure.figsize'] = (2.0, 2.0)\nimgplot = plt.imshow(img)\nimgplot.set_cmap('gray')  #you can experiment different colormaps (Greys,winter,autumn)"
        }, 
        {
            "source": "Now let's pass this image through the net:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "hh0 = tf.nn.sigmoid(tf.matmul(X, W) + hb)\nvv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\nfeed = sess.run(hh0, feed_dict={ X: sample_case, W: prv_w, hb: prv_hb})\nrec = sess.run(vv1, feed_dict={ hh0: feed, W: prv_w, vb: prv_vb})"
        }, 
        {
            "source": "Here we plot the reconstructed image:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "img = Image.fromarray(tile_raster_images(X=rec, img_shape=(28, 28),tile_shape=(1, 1), tile_spacing=(1, 1)))\nplt.rcParams['figure.figsize'] = (2.0, 2.0)\nimgplot = plt.imshow(img)\nimgplot.set_cmap('gray') "
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 2 with Spark 1.6 (Unsupported)", 
            "name": "python2", 
            "language": "python"
        }, 
        "widgets": {
            "state": {}, 
            "version": "1.1.2"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "2.7.11", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython2", 
            "codemirror_mode": {
                "version": 2, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}